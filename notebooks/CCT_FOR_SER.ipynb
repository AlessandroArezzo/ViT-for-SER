{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CCT_FOR_SER.ipynb",
   "provenance": [],
   "mount_file_id": "1jNeErBCJZlBIKsrUa2R6eMDAtPLVKRdU",
   "authorship_tag": "ABX9TyOhuEFyYzdZEVPwHM1xpuGK"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MOUNT DRIVE"
   ],
   "metadata": {
    "id": "4N2R6T1NFuVI"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QefZUEMXEQSc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655473045,
     "user_tz": -120,
     "elapsed": 1762,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "f17871ee-7078-46d9-ad9d-f982f2e08b5f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-d5df0069828e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdrive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/content/drive'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMPORT LIBRERIE"
   ],
   "metadata": {
    "id": "fmO0IEoxF0fG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pip install timm"
   ],
   "metadata": {
    "id": "jS-EEKbsF3Rf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655476224,
     "user_tz": -120,
     "elapsed": 3183,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "32e4e495-d4ba-4b8e-81cf-f152fed89eb6"
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages/vision-1.0.0-py3.7-nspkg.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"/home/ale-ubuntu/anaconda3/lib/python3.7/site.py\", line 168, in addpackage\r\n",
      "      exec(line)\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\r\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: timm in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (0.5.4)\r\n",
      "Requirement already satisfied: torchvision in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from timm) (0.12.0+cu113)\r\n",
      "Requirement already satisfied: torch>=1.4 in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from timm) (1.11.0+cu113)\r\n",
      "Requirement already satisfied: typing-extensions in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from torch>=1.4->timm) (3.7.4.3)\r\n",
      "Requirement already satisfied: numpy in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from torchvision->timm) (1.19.5)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from torchvision->timm) (8.1.0)\r\n",
      "Requirement already satisfied: requests in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from torchvision->timm) (2.25.1)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from requests->torchvision->timm) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from requests->torchvision->timm) (2021.10.8)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from requests->torchvision->timm) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (from requests->torchvision->timm) (1.26.3)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pip install einops"
   ],
   "metadata": {
    "id": "uuYYp_mCF7vm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480132,
     "user_tz": -120,
     "elapsed": 3914,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "e580cf20-4720-4369-bb9c-a6f652b67ca5"
   },
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages/vision-1.0.0-py3.7-nspkg.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"/home/ale-ubuntu/anaconda3/lib/python3.7/site.py\", line 168, in addpackage\r\n",
      "      exec(line)\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\r\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: einops in /home/ale-ubuntu/anaconda3/lib/python3.7/site-packages (0.3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HELPERS"
   ],
   "metadata": {
    "id": "RfIrNrL5IRU4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile helpers.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "_logger = logging.getLogger('train')\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1):\n",
    "    # Copied from `timm` by Ross Wightman:\n",
    "    # github.com/rwightman/pytorch-image-models\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    gs_new = int(math.sqrt(ntok_new))\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_new, gs_new), mode='bilinear')\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new * gs_new, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def pe_check(model, state_dict, pe_key='classifier.positional_emb'):\n",
    "    if pe_key is not None and pe_key in state_dict.keys() and pe_key in model.state_dict().keys():\n",
    "        if model.state_dict()[pe_key].shape != state_dict[pe_key].shape:\n",
    "            state_dict[pe_key] = resize_pos_embed(state_dict[pe_key],\n",
    "                                                  model.state_dict()[pe_key],\n",
    "                                                  num_tokens=model.classifier.num_tokens)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def fc_check(model, state_dict, fc_key='classifier.fc'):\n",
    "    for key in [f'{fc_key}.weight', f'{fc_key}.bias']:\n",
    "        if key is not None and key in state_dict.keys() and key in model.state_dict().keys():\n",
    "            if model.state_dict()[key].shape != state_dict[key].shape:\n",
    "                _logger.warning(f'Removing {key}, number of classes has changed.')\n",
    "                state_dict[key] = model.state_dict()[key]\n",
    "    return state_dict\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_0TgoMTITpQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480133,
     "user_tz": -120,
     "elapsed": 10,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "1070f188-93ed-48a2-cef1-ad1531bdb534"
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helpers.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def register_model(func):\n",
    "    \"\"\"\n",
    "    Fallback wrapper in case timm isn't installed\n",
    "    \"\"\"\n",
    "    return func"
   ],
   "metadata": {
    "id": "Bbzf_GZpI_7v",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480133,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile checkpoint.py\n",
    "\n",
    "import glob\n",
    "import operator\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "from timm.utils.model import unwrap_model, get_state_dict\n",
    "\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CheckpointSaver:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            optimizer,\n",
    "            args=None,\n",
    "            model_ema=None,\n",
    "            amp_scaler=None,\n",
    "            checkpoint_prefix='checkpoint',\n",
    "            recovery_prefix='recovery',\n",
    "            checkpoint_dir='',\n",
    "            recovery_dir='',\n",
    "            decreasing=False,\n",
    "            max_history=10,\n",
    "            unwrap_fn=unwrap_model):\n",
    "\n",
    "        # objects to save state_dicts of\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.args = args\n",
    "        self.model_ema = model_ema\n",
    "        self.amp_scaler = amp_scaler\n",
    "\n",
    "        # state\n",
    "        self.checkpoint_files = []  # (filename, metric) tuples in order of decreasing betterness\n",
    "        self.best_epoch = None\n",
    "        self.best_metric = None\n",
    "        self.curr_recovery_file = ''\n",
    "        self.last_recovery_file = ''\n",
    "\n",
    "        # config\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.recovery_dir = recovery_dir\n",
    "        self.save_prefix = checkpoint_prefix\n",
    "        self.recovery_prefix = recovery_prefix\n",
    "        self.extension = '.pth.tar'\n",
    "        self.decreasing = decreasing  # a lower metric is better if True\n",
    "        self.cmp = operator.lt if decreasing else operator.gt  # True if lhs better than rhs\n",
    "        self.max_history = max_history\n",
    "        self.unwrap_fn = unwrap_fn\n",
    "        assert self.max_history >= 1\n",
    "\n",
    "    def save_checkpoint(self, epoch, metric=None):\n",
    "        assert epoch >= 0\n",
    "        last_save_path = os.path.join(self.checkpoint_dir, 'last' + self.extension)\n",
    "        #self._save(last_save_path, epoch, metric)\n",
    "        worst_file = self.checkpoint_files[-1] if self.checkpoint_files else None\n",
    "        if (len(self.checkpoint_files) < self.max_history\n",
    "                or metric is None or self.cmp(metric, worst_file[1])):\n",
    "            if len(self.checkpoint_files) >= self.max_history:\n",
    "                self._cleanup_checkpoints(1)\n",
    "            filename = '-'.join([self.save_prefix, str(epoch)]) + self.extension\n",
    "            save_path = os.path.join(self.checkpoint_dir, filename)\n",
    "            #self._save(save_path, epoch, metric)\n",
    "            self.checkpoint_files.append((save_path, metric))\n",
    "            self.checkpoint_files = sorted(\n",
    "                self.checkpoint_files, key=lambda x: x[1],\n",
    "                reverse=not self.decreasing)  # sort in descending order if a lower metric is not better\n",
    "\n",
    "            if metric is not None and (self.best_metric is None or self.cmp(metric, self.best_metric)):\n",
    "              self.best_epoch = epoch\n",
    "              self.best_metric = metric\n",
    "              best_save_path = os.path.join(self.checkpoint_dir, 'model_best' + self.extension)\n",
    "              self._save(best_save_path, epoch, metric)\n",
    "\n",
    "            checkpoints_str = \"Current checkpoints:\\n\"\n",
    "            for c in self.checkpoint_files:\n",
    "                checkpoints_str += ' {}\\n'.format(c)\n",
    "            _logger.info(checkpoints_str)\n",
    "\n",
    "        return (None, None) if self.best_metric is None else (self.best_metric, self.best_epoch)\n",
    "\n",
    "    def _save(self, save_path, epoch, metric=None):\n",
    "        save_state = {\n",
    "            'epoch': epoch,\n",
    "            'arch': type(self.model).__name__.lower(),\n",
    "            'state_dict': get_state_dict(self.model, self.unwrap_fn),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'version': 2,  # version < 2 increments epoch before save\n",
    "        }\n",
    "        if self.args is not None:\n",
    "            save_state['arch'] = self.args.model\n",
    "            save_state['args'] = self.args\n",
    "        if self.amp_scaler is not None:\n",
    "            save_state[self.amp_scaler.state_dict_key] = self.amp_scaler.state_dict()\n",
    "        if self.model_ema is not None:\n",
    "            save_state['state_dict_ema'] = get_state_dict(self.model_ema, self.unwrap_fn)\n",
    "        if metric is not None:\n",
    "            save_state['metric'] = metric\n",
    "        torch.save(save_state, save_path)\n",
    "\n",
    "    def _cleanup_checkpoints(self, trim=0):\n",
    "        trim = min(len(self.checkpoint_files), trim)\n",
    "        delete_index = self.max_history - trim\n",
    "        if delete_index < 0 or len(self.checkpoint_files) <= delete_index:\n",
    "            return\n",
    "        to_delete = self.checkpoint_files[delete_index:]\n",
    "        for d in to_delete:\n",
    "            try:\n",
    "                _logger.debug(\"Cleaning checkpoint: {}\".format(d))\n",
    "                os.remove(d[0])\n",
    "            except Exception as e:\n",
    "                _logger.error(\"Exception '{}' while deleting checkpoint\".format(e))\n",
    "        self.checkpoint_files = self.checkpoint_files[:delete_index]\n",
    "\n",
    "    def save_recovery(self, epoch, batch_idx=0):\n",
    "        assert epoch >= 0\n",
    "        filename = '-'.join([self.recovery_prefix, str(epoch), str(batch_idx)]) + self.extension\n",
    "        save_path = os.path.join(self.recovery_dir, filename)\n",
    "        self._save(save_path, epoch)\n",
    "        if os.path.exists(self.last_recovery_file):\n",
    "            try:\n",
    "                _logger.debug(\"Cleaning recovery: {}\".format(self.last_recovery_file))\n",
    "                os.remove(self.last_recovery_file)\n",
    "            except Exception as e:\n",
    "                _logger.error(\"Exception '{}' while removing {}\".format(e, self.last_recovery_file))\n",
    "        self.last_recovery_file = self.curr_recovery_file\n",
    "        self.curr_recovery_file = save_path\n",
    "\n",
    "    def find_recovery(self):\n",
    "        recovery_path = os.path.join(self.recovery_dir, self.recovery_prefix)\n",
    "        files = glob.glob(recovery_path + '*' + self.extension)\n",
    "        files = sorted(files)\n",
    "        return files[0] if len(files) else ''\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urUbvZ04Pilk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480133,
     "user_tz": -120,
     "elapsed": 7,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "30c6cdf6-be07-4797-c103-762f8c54f6ab"
   },
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting checkpoint.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TOKENIZERS"
   ],
   "metadata": {
    "id": "ZaOtSGxVGJ8I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenizer\n",
    "\n"
   ],
   "metadata": {
    "id": "_oZ7DUIPGNTR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile tokenizer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size, stride, padding,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 n_conv_layers=1,\n",
    "                 n_input_channels=3,\n",
    "                 n_output_channels=64,\n",
    "                 in_planes=64,\n",
    "                 activation=None,\n",
    "                 max_pool=True,\n",
    "                 conv_bias=False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        n_filter_list = [n_input_channels] + \\\n",
    "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
    "                        [n_output_channels]\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
    "                          kernel_size=(kernel_size, kernel_size),\n",
    "                          stride=(stride, stride),\n",
    "                          padding=(padding, padding), bias=conv_bias),\n",
    "                nn.Identity() if activation is None else activation(),\n",
    "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
    "                             stride=pooling_stride,\n",
    "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
    "            )\n",
    "                for i in range(n_conv_layers)\n",
    "            ])\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class TextTokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size, stride, padding,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 embedding_dim=300,\n",
    "                 n_output_channels=128,\n",
    "                 activation=None,\n",
    "                 max_pool=True,\n",
    "                 *args, **kwargs):\n",
    "        super(TextTokenizer, self).__init__()\n",
    "\n",
    "        self.max_pool = max_pool\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, n_output_channels,\n",
    "                      kernel_size=(kernel_size, embedding_dim),\n",
    "                      stride=(stride, 1),\n",
    "                      padding=(padding, 0), bias=False),\n",
    "            nn.Identity() if activation is None else activation(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=(pooling_kernel_size, 1),\n",
    "                stride=(pooling_stride, 1),\n",
    "                padding=(pooling_padding, 0)\n",
    "            ) if max_pool else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def seq_len(self, seq_len=32, embed_dim=300):\n",
    "        return self.forward(torch.zeros((1, seq_len, embed_dim)))[0].shape[1]\n",
    "\n",
    "    def forward_mask(self, mask):\n",
    "        new_mask = mask.unsqueeze(1).float()\n",
    "        cnn_weight = torch.ones(\n",
    "            (1, 1, self.conv_layers[0].kernel_size[0]),\n",
    "            device=mask.device,\n",
    "            dtype=torch.float)\n",
    "        new_mask = F.conv1d(\n",
    "            new_mask, cnn_weight, None,\n",
    "            self.conv_layers[0].stride[0], self.conv_layers[0].padding[0], 1, 1)\n",
    "        if self.max_pool:\n",
    "            new_mask = F.max_pool1d(\n",
    "                new_mask, self.conv_layers[2].kernel_size[0],\n",
    "                self.conv_layers[2].stride[0], self.conv_layers[2].padding[0], 1, False, False)\n",
    "        new_mask = new_mask.squeeze(1)\n",
    "        new_mask = (new_mask > 0)\n",
    "        return new_mask\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.transpose(1, 3).squeeze(1)\n",
    "        if mask is not None:\n",
    "            mask = self.forward_mask(mask).unsqueeze(-1).float()\n",
    "            x = x * mask\n",
    "        return x, mask\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n"
   ],
   "metadata": {
    "id": "TW-tYumPF88u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480133,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "ca26e2ea-3975-4f94-92b9-7d7f81d515b5"
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tokenizer.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Memory Tokenizer"
   ],
   "metadata": {
    "id": "0SdBA36CGt8O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch\n",
    "\n",
    "class VerticalTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains the module of the ViT that generates vertical patch embeddings from the input image.\n",
    "    Attributes:\n",
    "    projection        object of nn.Sequential class that splits the image into patches\n",
    "                      and generates projections in the size of the embedding\n",
    "    positions         object of nn.Parameter class that generates the positional encoding vectors to add to all patch\n",
    "                      embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, width_patches, dim, channels=3):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=width_patches, p2=img_size),\n",
    "            nn.Linear(channels * img_size * width_patches, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Sequential):\n",
    "            nn.init.kaiming_normal_(m.weight)"
   ],
   "metadata": {
    "id": "z4qkuIwMGtpI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480134,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRANSFORMERS"
   ],
   "metadata": {
    "id": "BCChviQuG4Rn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile transformers_component.py\n",
    "\n",
    "# Thanks to rwightman's timm package\n",
    "# github.com:rwightman/pytorch-image-models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Identity, Parameter, init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Obtained from: github.com:rwightman/pytorch-image-models\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Module):\n",
    "    \"\"\"\n",
    "    Obtained from timm: github.com:rwightman/pytorch-image-models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = Dropout(attention_dropout)\n",
    "        self.proj = Linear(dim, dim)\n",
    "        self.proj_drop = Dropout(projection_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskedAttention(Module):\n",
    "    def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = Dropout(attention_dropout)\n",
    "        self.proj = Linear(dim, dim)\n",
    "        self.proj_drop = Dropout(projection_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_value = -torch.finfo(attn.dtype).max\n",
    "            assert mask.shape[-1] == attn.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "            attn.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    \"\"\"\n",
    "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.pre_norm = LayerNorm(d_model)\n",
    "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
    "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
    "        src = src + self.drop_path(self.dropout2(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class MaskedTransformerEncoderLayer(Module):\n",
    "    \"\"\"\n",
    "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
    "        super(MaskedTransformerEncoderLayer, self).__init__()\n",
    "        self.pre_norm = LayerNorm(d_model)\n",
    "        self.self_attn = MaskedAttention(dim=d_model, num_heads=nhead,\n",
    "                                         attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, mask=None, *args, **kwargs) -> torch.Tensor:\n",
    "        src = src + self.drop_path(self.self_attn(self.pre_norm(src), mask))\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
    "        src = src + self.drop_path(self.dropout2(src2))\n",
    "        return src\n",
    "\n",
    "\n",
    "class TransformerClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 seq_pool=True,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seq_pool = seq_pool\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        if not seq_pool:\n",
    "            sequence_length += 1\n",
    "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
    "                                       requires_grad=True)\n",
    "            self.num_tokens = 1\n",
    "        else:\n",
    "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if not self.seq_pool:\n",
    "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.seq_pool:\n",
    "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "\n",
    "class MaskedTransformerClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 seq_pool=True,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='sine',\n",
    "                 seq_len=None,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.seq_pool = seq_pool\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert seq_len is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        if not seq_pool:\n",
    "            seq_len += 1\n",
    "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
    "                                       requires_grad=True)\n",
    "            self.num_tokens = 1\n",
    "        else:\n",
    "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                seq_len += 1  # padding idx\n",
    "                self.positional_emb = Parameter(torch.zeros(1, seq_len, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(seq_len,\n",
    "                                                                          embedding_dim,\n",
    "                                                                          padding_idx=True),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            MaskedTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                          dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                          attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.positional_emb is None and x.size(1) < self.seq_len:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if not self.seq_pool:\n",
    "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "            if mask is not None:\n",
    "                mask = torch.cat([torch.ones(size=(mask.shape[0], 1), device=mask.device), mask.float()], dim=1)\n",
    "                mask = (mask > 0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=mask)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.seq_pool:\n",
    "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim, padding_idx=False):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        pe = pe.unsqueeze(0)\n",
    "        if padding_idx:\n",
    "            return torch.cat([torch.zeros((1, 1, dim)), pe], dim=1)\n",
    "        return pe\n",
    "\n"
   ],
   "metadata": {
    "id": "FzuI4OWzG1nP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480134,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "bf9f58c8-f90e-4e8c-b2ac-7f4b7c39de6f"
   },
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformers_component.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer Memory"
   ],
   "metadata": {
    "id": "HL1_bYtWKv-L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn import Module\n",
    "class TransformerMemoryClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        self.attention_pool = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.lstm = LSTM(embedding_dim, embedding_dim)\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "        self.fc = Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def extract_embedding(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extract_memory_embedding(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "kPOkpgEqKvvo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480620,
     "user_tz": -120,
     "elapsed": 492,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer Parallel Memory Classifier"
   ],
   "metadata": {
    "id": "tiZCzPvUMeYx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class ParallelMemoryClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None,\n",
    "                 type='concatenate'):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        assert type == 'concatenate' or type == 'sum' or type == 'average', \\\n",
    "            f\"Union output type is set to {type} that is not specified\"\n",
    "        self.union_type = type\n",
    "\n",
    "        self.attention_pool_cct = Linear(self.embedding_dim, 1)\n",
    "        self.attention_pool_lstm = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm_cct = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lstm = LSTM(embedding_dim, embedding_dim)\n",
    "        self.norm_lstm = LayerNorm(embedding_dim)\n",
    "        fc_dim = embedding_dim\n",
    "        if self.union_type == 'concatenate':\n",
    "            fc_dim = embedding_dim * 2\n",
    "\n",
    "        self.norm_union = LayerNorm(fc_dim)\n",
    "        self.fc = Linear(fc_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x_cct, x_lstm):\n",
    "        if self.positional_emb is None and x_cct.size(1) < self.sequence_length:\n",
    "            x_cct = F.pad(x_cct, (0, 0, 0, self.n_channels - x_cct.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x_cct += self.positional_emb\n",
    "\n",
    "        x_cct = self.dropout(x_cct)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x_cct = blk(x_cct)\n",
    "\n",
    "        x_cct = self.norm_cct(x_cct)\n",
    "        x_cct = torch.matmul(F.softmax(self.attention_pool_cct(x_cct), dim=1).transpose(-1, -2), x_cct).squeeze(-2)\n",
    "\n",
    "        x_lstm = self.lstm(x_lstm)[0]\n",
    "        x_lstm = self.norm_lstm(x_lstm)\n",
    "\n",
    "        x_lstm = torch.matmul(F.softmax(self.attention_pool_lstm(x_lstm), dim=1).transpose(-1, -2), x_lstm).squeeze(-2)\n",
    "        if self.union_type == 'concatenate':\n",
    "            x = torch.cat((x_cct, x_lstm), 1)\n",
    "        elif self.union_type == 'sum':\n",
    "            x = torch.sum(torch.stack([x_cct, x_lstm]), dim=0)\n",
    "        elif self.union_type == 'average':\n",
    "            x = torch.mean(torch.stack([x_cct, x_lstm]), dim=0)\n",
    "\n",
    "        x = self.norm_union(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)"
   ],
   "metadata": {
    "id": "D3zVzHPuMhtx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480621,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ViT\n"
   ],
   "metadata": {
    "id": "n5CXlkMKIqNf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "model_urls = {\n",
    "}\n",
    "\n",
    "\n",
    "class ViTLite(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 kernel_size=16,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(ViTLite, self).__init__()\n",
    "        assert img_size % kernel_size == 0, f\"Image size ({img_size}) has to be\" \\\n",
    "                                            f\"divisible by patch size ({kernel_size})\"\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=kernel_size,\n",
    "                                   padding=0,\n",
    "                                   max_pool=False,\n",
    "                                   activation=None,\n",
    "                                   n_conv_layers=1,\n",
    "                                   conv_bias=True)\n",
    "\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            seq_pool=False,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def _vit_lite(arch, pretrained, progress,\n",
    "              num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "              kernel_size=4, *args, **kwargs):\n",
    "    model = ViTLite(num_layers=num_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    positional_embedding='learnable',\n",
    "                    *args, **kwargs)\n",
    "\n",
    "    if pretrained and arch in model_urls:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        if positional_embedding == 'learnable':\n",
    "            state_dict = pe_check(model, state_dict)\n",
    "        elif positional_embedding == 'sine':\n",
    "            state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "        state_dict = pe_check(model, state_dict)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_2(*args, **kwargs):\n",
    "    return _vit_lite(num_layers=2, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                     *args, **kwargs)\n",
    "\n",
    "\n",
    "def vit_4(*args, **kwargs):\n",
    "    return _vit_lite(num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                     *args, **kwargs)\n",
    "\n",
    "\n",
    "def vit_6(*args, **kwargs):\n",
    "    return _vit_lite(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                     *args, **kwargs)\n",
    "\n",
    "\n",
    "def vit_7(*args, **kwargs):\n",
    "    return _vit_lite(num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                     *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_2_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return vit_2('vit_2_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_2_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return vit_2('vit_2_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_4_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return vit_4('vit_4_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_4_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return vit_4('vit_4_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_6_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return vit_6('vit_6_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_6_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return vit_6('vit_6_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_7_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return vit_7('vit_7_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_7_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return vit_7('vit_7_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n"
   ],
   "metadata": {
    "id": "WKqZ4TuwIvV4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655480621,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CVT"
   ],
   "metadata": {
    "id": "6cmS4fpGJFWW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "model_urls = {\n",
    "}\n",
    "\n",
    "\n",
    "class CVT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 kernel_size=16,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(CVT, self).__init__()\n",
    "        assert img_size % kernel_size == 0, f\"Image size ({img_size}) has to be\" \\\n",
    "                                            f\"divisible by patch size ({kernel_size})\"\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=kernel_size,\n",
    "                                   padding=0,\n",
    "                                   max_pool=False,\n",
    "                                   activation=None,\n",
    "                                   n_conv_layers=1,\n",
    "                                   conv_bias=True)\n",
    "\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            seq_pool=True,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def _cvt(arch, pretrained, progress,\n",
    "         num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "         kernel_size=4, positional_embedding='learnable',\n",
    "         *args, **kwargs):\n",
    "    model = CVT(num_layers=num_layers,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                embedding_dim=embedding_dim,\n",
    "                kernel_size=kernel_size,\n",
    "                *args, **kwargs)\n",
    "\n",
    "    if pretrained and arch in model_urls:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        if positional_embedding == 'learnable':\n",
    "            state_dict = pe_check(model, state_dict)\n",
    "        elif positional_embedding == 'sine':\n",
    "            state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def cvt_2(*args, **kwargs):\n",
    "    return _cvt(num_layers=2, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cvt_4(*args, **kwargs):\n",
    "    return _cvt(num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cvt_6(*args, **kwargs):\n",
    "    return _cvt(num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cvt_7(*args, **kwargs):\n",
    "    return _cvt(num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cvt_8(*args, **kwargs):\n",
    "    return _cvt(num_layers=8, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_2_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return cvt_2('cvt_2_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_2_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return cvt_2('cvt_2_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_4_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return cvt_4('cvt_4_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_4_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return cvt_4('cvt_4_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_6_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return cvt_6('cvt_6_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_6_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return cvt_6('cvt_6_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_7_4_32(pretrained=False, progress=False,\n",
    "               img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "               *args, **kwargs):\n",
    "    return cvt_7('cvt_7_4_32', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cvt_7_4_32_sine(pretrained=False, progress=False,\n",
    "                    img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                    *args, **kwargs):\n",
    "    return cvt_7('cvt_7_4_32_sine', pretrained, progress,\n",
    "                 kernel_size=4,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)"
   ],
   "metadata": {
    "id": "G2Z7N4kQJHee",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655481099,
     "user_tz": -120,
     "elapsed": 480,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CCT"
   ],
   "metadata": {
    "id": "kx49zehHJRXf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile cct.py\n",
    "\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.nn as nn\n",
    "from tokenizer import Tokenizer\n",
    "from transformers_component import TransformerEncoderLayer, TransformerClassifier\n",
    "from helpers import pe_check, fc_check\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "model_urls = {\n",
    "    'cct_7_3x1_32':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar10_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar10_5000epochs.pth',\n",
    "    'cct_7_3x1_32_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar100_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar100_5000epochs.pth',\n",
    "    'cct_7_7x2_224_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_7x2_224_flowers102.pth',\n",
    "    'cct_14_7x2_224':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth',\n",
    "    'cct_14_7x2_384':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_imagenet.pth',\n",
    "    'cct_14_7x2_384_fl':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_flowers102.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class CCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 n_conv_layers=1,\n",
    "                 kernel_size=7,\n",
    "                 stride=2,\n",
    "                 padding=3,\n",
    "                 pooling_kernel_size=3,\n",
    "                 pooling_stride=2,\n",
    "                 pooling_padding=1,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(CCT, self).__init__()\n",
    "\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   pooling_kernel_size=pooling_kernel_size,\n",
    "                                   pooling_stride=pooling_stride,\n",
    "                                   pooling_padding=pooling_padding,\n",
    "                                   max_pool=True,\n",
    "                                   activation=nn.ReLU,\n",
    "                                   n_conv_layers=n_conv_layers,\n",
    "                                   conv_bias=False)\n",
    "\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            seq_pool=True,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def _cct(arch, pretrained, progress,\n",
    "         num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "         kernel_size=3, stride=None, padding=None,\n",
    "         positional_embedding='learnable',\n",
    "         *args, **kwargs):\n",
    "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
    "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
    "    model = CCT(num_layers=num_layers,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                embedding_dim=embedding_dim,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                *args, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        if arch in model_urls:\n",
    "            state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                                  progress=progress)\n",
    "            if positional_embedding == 'learnable':\n",
    "                state_dict = pe_check(model, state_dict)\n",
    "            elif positional_embedding == 'sine':\n",
    "                state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "            state_dict = fc_check(model, state_dict)\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            raise RuntimeError(f'Variant {arch} does not yet have pretrained weights.')\n",
    "    return model\n",
    "\n",
    "\n",
    "def cct_2(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _cct(arch, pretrained, progress, num_layers=2, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cct_4(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _cct(arch, pretrained, progress, num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cct_6(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _cct(arch, pretrained, progress, num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cct_7(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _cct(arch, pretrained, progress, num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                *args, **kwargs)\n",
    "\n",
    "\n",
    "def cct_14(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _cct(arch, pretrained, progress, num_layers=14, num_heads=6, mlp_ratio=3, embedding_dim=384,\n",
    "                *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def cct_2_3x2_32(pretrained=False, progress=False,\n",
    "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                 *args, **kwargs):\n",
    "    return cct_2('cct_2_3x2_32', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_2_3x2_32_sine(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return cct_2('cct_2_3x2_32_sine', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_4_3x2_32(pretrained=False, progress=False,\n",
    "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                 *args, **kwargs):\n",
    "    return cct_4('cct_4_3x2_32', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_4_3x2_32_sine(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return cct_4('cct_4_3x2_32_sine', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_6_3x1_32(pretrained=False, progress=False,\n",
    "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                 *args, **kwargs):\n",
    "    return cct_6('cct_6_3x1_32', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=1,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_6_3x1_32_sine(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return cct_6('cct_6_3x1_32_sine', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=1,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_6_3x2_32(pretrained=False, progress=False,\n",
    "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                 *args, **kwargs):\n",
    "    return cct_6('cct_6_3x2_32', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_6_3x2_32_sine(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return cct_6('cct_6_3x2_32_sine', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_3x1_32(pretrained=False, progress=False,\n",
    "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                 *args, **kwargs):\n",
    "    return cct_7('cct_7_3x1_32', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=1,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def cct_7_3x1_32_sine(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return cct_7('cct_7_3x1_32_sine', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=1,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_3x1_32_c100(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='learnable', num_classes=100,\n",
    "                      *args, **kwargs):\n",
    "    return cct_7('cct_7_3x1_32_c100', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=1,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_3x1_32_sine_c100(pretrained=False, progress=False,\n",
    "                           img_size=32, positional_embedding='sine', num_classes=100,\n",
    "                           *args, **kwargs):\n",
    "    return cct_7('cct_7_3x1_32_sine_c100', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=1,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_3x2_32(pretrained=False, progress=False,\n",
    "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                 *args, **kwargs):\n",
    "    return cct_7('cct_7_3x2_32', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_3x2_32_sine(pretrained=False, progress=False,\n",
    "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return cct_7('cct_7_3x2_32_sine', pretrained, progress,\n",
    "                 kernel_size=3, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_7x2_224(pretrained=False, progress=False,\n",
    "                  img_size=224, positional_embedding='learnable', num_classes=102,\n",
    "                  *args, **kwargs):\n",
    "    return cct_7('cct_7_7x2_224', pretrained, progress,\n",
    "                 kernel_size=7, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_7_7x2_224_sine(pretrained=False, progress=False,\n",
    "                       img_size=224, positional_embedding='sine', num_classes=102,\n",
    "                       *args, **kwargs):\n",
    "    return cct_7('cct_7_7x2_224_sine', pretrained, progress,\n",
    "                 kernel_size=7, n_conv_layers=2,\n",
    "                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                 num_classes=num_classes,\n",
    "                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_14_7x2_224(pretrained=False, progress=False,\n",
    "                   img_size=224, positional_embedding='learnable', num_classes=1000,\n",
    "                   *args, **kwargs):\n",
    "    return cct_14('cct_14_7x2_224', pretrained, progress,\n",
    "                  kernel_size=7, n_conv_layers=2,\n",
    "                  img_size=img_size, positional_embedding=positional_embedding,\n",
    "                  num_classes=num_classes,\n",
    "                  *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_14_7x2_384(pretrained=False, progress=False,\n",
    "                   img_size=384, positional_embedding='learnable', num_classes=1000,\n",
    "                   *args, **kwargs):\n",
    "    return cct_14('cct_14_7x2_384', pretrained, progress,\n",
    "                  kernel_size=7, n_conv_layers=2,\n",
    "                  img_size=img_size, positional_embedding=positional_embedding,\n",
    "                  num_classes=num_classes,\n",
    "                  *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def cct_14_7x2_384_fl(pretrained=False, progress=False,\n",
    "                      img_size=384, positional_embedding='learnable', num_classes=102,\n",
    "                      *args, **kwargs):\n",
    "    return cct_14('cct_14_7x2_384_fl', pretrained, progress,\n",
    "                  kernel_size=7, n_conv_layers=2,\n",
    "                  img_size=img_size, positional_embedding=positional_embedding,\n",
    "                  num_classes=num_classes,\n",
    "                  *args, **kwargs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGgtZSHPJTAb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655481100,
     "user_tz": -120,
     "elapsed": 7,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "79b06448-2898-494f-c2b7-4c8f1c493d89"
   },
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cct.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MCVT"
   ],
   "metadata": {
    "id": "WeM0WUEwJ2_X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "class MemoryCVT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 width_patches=2,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(MemoryCVT, self).__init__()\n",
    "        assert img_size % width_patches == 0, f\"Image size ({img_size}) has to be\" \\\n",
    "                                              f\"divisible by patch size ({width_patches})\"\n",
    "        self.tokenizer = VerticalTokenizer(img_size=img_size, channels=n_input_channels, dim=embedding_dim,\n",
    "                                           width_patches=width_patches)\n",
    "\n",
    "        self.classifier = TransformerMemoryClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def _memory_cvt(num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "                width_patches=2,\n",
    "                *args, **kwargs):\n",
    "    model = MemoryCVT(num_layers=num_layers,\n",
    "                      num_heads=num_heads,\n",
    "                      mlp_ratio=mlp_ratio,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      width_patches=width_patches,\n",
    "                      *args, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def memory_cvt_4(*args, **kwargs):\n",
    "    return _memory_cvt(num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                       *args, **kwargs)\n",
    "\n",
    "\n",
    "def memory_cvt_7(*args, **kwargs):\n",
    "    return _memory_cvt(num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                       *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mcvt_4_1_32(\n",
    "        img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "        *args, **kwargs):\n",
    "    return memory_cvt_4(arch='mcvt_4_1_32',\n",
    "                        width_patches=1,\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mcvt_4_2_32(img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                *args, **kwargs):\n",
    "    return memory_cvt_4(arch='mcvt_4_2_32',\n",
    "                        width_patches=2,\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mcvt_7_1_32(img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                *args, **kwargs):\n",
    "    return memory_cvt_7(arch='mcvt_7_1_32',\n",
    "                        width_patches=1,\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mcvt_7_2_32(img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                *args, **kwargs):\n",
    "    return memory_cvt_7(arch='mcvt_7_2_32',\n",
    "                        width_patches=2,\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n"
   ],
   "metadata": {
    "id": "a75TZLeMJ2ev",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655481100,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MCCT"
   ],
   "metadata": {
    "id": "M3CWTbeeKVSn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Parameter, init, LSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'cct_7_3x1_32':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar10_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar10_5000epochs.pth',\n",
    "    'cct_7_3x1_32_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar100_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar100_5000epochs.pth',\n",
    "    'cct_7_7x2_224_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_7x2_224_flowers102.pth',\n",
    "    'cct_14_7x2_224':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth',\n",
    "    'cct_14_7x2_384':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_imagenet.pth',\n",
    "    'cct_14_7x2_384_fl':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_flowers102.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class MemoryCCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 n_conv_layers=1,\n",
    "                 kernel_size=7,\n",
    "                 stride=2,\n",
    "                 padding=3,\n",
    "                 pooling_kernel_size=3,\n",
    "                 pooling_stride=2,\n",
    "                 pooling_padding=1,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(MemoryCCT, self).__init__()\n",
    "\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   pooling_kernel_size=pooling_kernel_size,\n",
    "                                   pooling_stride=pooling_stride,\n",
    "                                   pooling_padding=pooling_padding,\n",
    "                                   max_pool=True,\n",
    "                                   activation=nn.ReLU,\n",
    "                                   n_conv_layers=n_conv_layers,\n",
    "                                   conv_bias=False)\n",
    "\n",
    "        self.classifier = TransformerMemoryClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "    def extract_embedding(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier.extract_embedding(x)\n",
    "\n",
    "    def extract_memory_embedding(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier.extract_memory_embedding(x)\n",
    "\n",
    "\n",
    "def _memory_cct(arch, pretrained, progress, pretrained_arch,\n",
    "                num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "                kernel_size=3, stride=None, padding=None, positional_embedding='learnable',\n",
    "                *args, **kwargs):\n",
    "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
    "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
    "    model = MemoryCCT(num_layers=num_layers,\n",
    "                      num_heads=num_heads,\n",
    "                      mlp_ratio=mlp_ratio,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      *args, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        if pretrained_arch in model_urls:\n",
    "            state_dict = load_state_dict_from_url(model_urls[pretrained_arch],\n",
    "                                                  progress=progress)\n",
    "            if positional_embedding == 'learnable':\n",
    "                state_dict = pe_check(model, state_dict)\n",
    "            elif positional_embedding == 'sine':\n",
    "                state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v for k, v in state_dict.items() if\n",
    "                                k in model_dict and v.size() == model_dict[k].size()}\n",
    "            model_dict.update(state_dict)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        else:\n",
    "            raise RuntimeError(f'Variant {pretrained_arch} does not yet have pretrained weights.')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def memory_cct_7(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _memory_cct(arch=arch, pretrained=pretrained, progress=progress,\n",
    "                       num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                       *args, **kwargs)\n",
    "\n",
    "\n",
    "def memory_cct_4(arch, pretrained, progress, *args, **kwargs):\n",
    "    return _memory_cct(arch=arch, pretrained=pretrained, progress=progress,\n",
    "                       num_layers=4, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                       *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mcct_4_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                  *args, **kwargs):\n",
    "    return memory_cct_4(arch='mcct_4_3x1_32', pretrained=pretrained, progress=progress,\n",
    "                        kernel_size=3, n_conv_layers=1,  pretrained_arch=\"\",\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def mcct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                  *args, **kwargs):\n",
    "    return memory_cct_7(arch='mcct_7_3x1_32', pretrained=pretrained, progress=progress, pretrained_arch=\"cct_7_3x1_32\",\n",
    "                        kernel_size=3, n_conv_layers=1,\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def mcct_7_3x1_32_c100(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                  *args, **kwargs):\n",
    "    return memory_cct_7(arch='mcct_7_3x1_32_c100', pretrained=pretrained, progress=progress, pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                        kernel_size=3, n_conv_layers=1,\n",
    "                        img_size=img_size, positional_embedding=positional_embedding,\n",
    "                        num_classes=num_classes,\n",
    "                        *args, **kwargs)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "-77t0XI4KYYs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655481409,
     "user_tz": -120,
     "elapsed": 315,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# P_MCCT"
   ],
   "metadata": {
    "id": "EeSfrCq3MCU_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, Linear, Dropout, LayerNorm, Parameter, init, LSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "model_urls = {\n",
    "    'cct_7_3x1_32':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar10_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar10_5000epochs.pth',\n",
    "    'cct_7_3x1_32_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar100_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar100_5000epochs.pth',\n",
    "    'cct_7_7x2_224_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_7x2_224_flowers102.pth',\n",
    "    'cct_14_7x2_224':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth',\n",
    "    'cct_14_7x2_384':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_imagenet.pth',\n",
    "    'cct_14_7x2_384_fl':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_flowers102.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class ParallelMemoryCCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 n_conv_layers=1,\n",
    "                 kernel_size=7,\n",
    "                 stride=2,\n",
    "                 padding=3,\n",
    "                 pooling_kernel_size=3,\n",
    "                 pooling_stride=2,\n",
    "                 pooling_padding=1,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 width_patches_lstm=2,\n",
    "                 type='concatenate',  # type of union lstm output with cct output: concatenate | sum | average\n",
    "                 *args, **kwargs):\n",
    "        super(ParallelMemoryCCT, self).__init__()\n",
    "\n",
    "        self.tokenizer_cct = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                       n_output_channels=embedding_dim,\n",
    "                                       kernel_size=kernel_size,\n",
    "                                       stride=stride,\n",
    "                                       padding=padding,\n",
    "                                       pooling_kernel_size=pooling_kernel_size,\n",
    "                                       pooling_stride=pooling_stride,\n",
    "                                       pooling_padding=pooling_padding,\n",
    "                                       max_pool=True,\n",
    "                                       activation=nn.ReLU,\n",
    "                                       n_conv_layers=n_conv_layers,\n",
    "                                       conv_bias=False)\n",
    "\n",
    "        self.tokenizer_lstm = VerticalTokenizer(img_size=img_size, channels=n_input_channels, dim=embedding_dim,\n",
    "                                                width_patches=width_patches_lstm)\n",
    "\n",
    "        self.classifier = ParallelMemoryClassifier(\n",
    "            sequence_length=self.tokenizer_cct.sequence_length(n_channels=n_input_channels,\n",
    "                                                               height=img_size,\n",
    "                                                               width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding,\n",
    "            type=type\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cct = self.tokenizer_cct(x)\n",
    "        x_lstm = self.tokenizer_lstm(x)\n",
    "\n",
    "        return self.classifier(x_cct, x_lstm)\n",
    "\n",
    "\n",
    "\n",
    "def _parallel_memory_cct(arch, pretrained, progress, pretrained_arch, num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "                         kernel_size=3, stride=None, padding=None, positional_embedding='learnable',\n",
    "                         width_patches=2, type='concatenate',\n",
    "                         *args, **kwargs):\n",
    "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
    "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
    "    model = ParallelMemoryCCT(num_layers=num_layers,\n",
    "                              num_heads=num_heads,\n",
    "                              mlp_ratio=mlp_ratio,\n",
    "                              embedding_dim=embedding_dim,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=stride,\n",
    "                              padding=padding, width_patches_lstm=width_patches,\n",
    "                              type=type,\n",
    "                              *args, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        if pretrained_arch in model_urls:\n",
    "            state_dict = load_state_dict_from_url(model_urls[pretrained_arch],\n",
    "                                                  progress=progress)\n",
    "            if positional_embedding == 'learnable':\n",
    "                state_dict = pe_check(model, state_dict)\n",
    "            elif positional_embedding == 'sine':\n",
    "                state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v for k, v in state_dict.items() if\n",
    "                                k in model_dict and v.size() == model_dict[k].size()}\n",
    "            model_dict.update(state_dict)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        else:\n",
    "            raise RuntimeError(f'Variant {pretrained_arch} does not yet have pretrained weights.')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def parallel_memory_cct_4(arch, pretrained, progress, width_patches, type, *args, **kwargs):\n",
    "    return _parallel_memory_cct(arch=arch, pretrained=pretrained, progress=progress,\n",
    "                                num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                                width_patches=width_patches, type=type, *args, **kwargs)\n",
    "\n",
    "\n",
    "def parallel_memory_cct_7(arch, pretrained, progress, width_patches, type, *args, **kwargs):\n",
    "    return _parallel_memory_cct(arch=arch, pretrained=pretrained, progress=progress,\n",
    "                                num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                                width_patches=width_patches, type=type, *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_1_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_1_3x1_32', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_1_3x1_32_c100(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_1_3x1_32_c100', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_1_3x1_32_sum(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                          *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_1_3x1_32_sum',\n",
    "                                 pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='sum',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_1_3x1_32_average(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                              *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_1_3x1_32_average', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='average',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_2_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_2_3x1_32', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_2_3x1_32_c100(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_2_3x1_32_c100', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_2_3x1_32_sum(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                          *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_2_3x1_32_sum', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='sum',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_7_2_3x1_32_average(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                              *args, **kwargs):\n",
    "    return parallel_memory_cct_7(arch='p_mcct_7_2_3x1_32_average', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"cct_7_3x1_32\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='average',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_4_1_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return parallel_memory_cct_4(arch='p_mcct_4_1_3x1_32', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_4_1_3x1_32_sum(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                          *args, **kwargs):\n",
    "    return parallel_memory_cct_4(arch='p_mcct_4_1_3x1_32_sum', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='sum',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_4_1_3x1_32_average(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                              *args, **kwargs):\n",
    "    return parallel_memory_cct_4(arch='p_mcct_4_1_3x1_32_average', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='average',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_4_2_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                      *args, **kwargs):\n",
    "    return parallel_memory_cct_4(arch='p_mcct_4_2_3x1_32', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_4_2_3x1_32_sum(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                          *args, **kwargs):\n",
    "    return parallel_memory_cct_4(arch='p_mcct_4_2_3x1_32_sum', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='sum',\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def p_mcct_4_2_3x1_32_average(pretrained=False, progress=False, img_size=32, positional_embedding='learnable',\n",
    "                              num_classes=10, *args, **kwargs):\n",
    "    return parallel_memory_cct_4(arch='p_mcct_4_2_3x1_32_average', pretrained=pretrained, progress=progress,\n",
    "                                 pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1, width_patches=2,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='average',\n",
    "                                 *args, **kwargs)"
   ],
   "metadata": {
    "id": "t5pFvTXhMF03",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655481815,
     "user_tz": -120,
     "elapsed": 408,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Speaker CCT"
   ],
   "metadata": {
    "id": "mCvRbwcxMuFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "from timm.models.helpers import load_state_dict\n",
    "from timm.models.layers import set_layer_config\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Module, ModuleList, Linear, Dropout, LayerNorm, Parameter, init, LSTM\n",
    "import torch.nn.functional as F\n",
    "#import wavencoder\n",
    "import librosa\n",
    "#from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model, model_entrypoint\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "model_urls = {\n",
    "    'cct_7_3x1_32':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar10_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar10_5000epochs.pth',\n",
    "    'cct_7_3x1_32_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar100_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar100_5000epochs.pth',\n",
    "    'cct_7_7x2_224_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_7x2_224_flowers102.pth',\n",
    "    'cct_14_7x2_224':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth',\n",
    "    'cct_14_7x2_384':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_imagenet.pth',\n",
    "    'cct_14_7x2_384_fl':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_flowers102.pth',\n",
    "}\n",
    "\n",
    "class SpeakerCCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 speaker_embedder=None,\n",
    "                 gender_embedder=None,\n",
    "                 corpus_embedder=None,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 n_conv_layers=1,\n",
    "                 kernel_size=7,\n",
    "                 stride=2,\n",
    "                 padding=3,\n",
    "                 pooling_kernel_size=3,\n",
    "                 pooling_stride=2,\n",
    "                 pooling_padding=1,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 type='self-attention',\n",
    "                 n_layers_scnd_transformer=4,\n",
    "                 n_heads_scnd_transformer=2,\n",
    "                 after_fc_embedded=False,\n",
    "                 *args, **kwargs):\n",
    "        super(SpeakerCCT, self).__init__()\n",
    "\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   pooling_kernel_size=pooling_kernel_size,\n",
    "                                   pooling_stride=pooling_stride,\n",
    "                                   pooling_padding=pooling_padding,\n",
    "                                   max_pool=True,\n",
    "                                   activation=nn.ReLU,\n",
    "                                   n_conv_layers=n_conv_layers,\n",
    "                                   conv_bias=False)\n",
    "\n",
    "        self.speaker_embedder = speaker_embedder\n",
    "        self.gender_embedder = gender_embedder\n",
    "        self.corpus_embedder = corpus_embedder\n",
    "\n",
    "        if self.speaker_embedder is not None:\n",
    "            self.speaker_embedder.requires_grad = False\n",
    "        if self.gender_embedder is not None:\n",
    "            self.gender_embedder.requires_grad = False\n",
    "        if self.corpus_embedder is not None:\n",
    "            self.corpus_embedder.requires_grad = False\n",
    "        self.after_fc_embedded=after_fc_embedded\n",
    "\n",
    "        self.classifier = SpeakerCCTClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding,\n",
    "            type=type,\n",
    "            n_layers_scnd_transformer=n_layers_scnd_transformer,\n",
    "            n_heads_scnd_transformer=n_heads_scnd_transformer,\n",
    "            after_fc_embedded=after_fc_embedded\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_spect = self.tokenizer(x)\n",
    "        x_speaker = None\n",
    "        if self.speaker_embedder:\n",
    "            if self.after_fc_embedded:\n",
    "                x_speaker = self.speaker_embedder(x)\n",
    "            else:\n",
    "                x_speaker = self.speaker_embedder.extract_memory_embedding(x)\n",
    "        x_gender = None\n",
    "        if self.gender_embedder:\n",
    "            if self.after_fc_embedded:\n",
    "                x_gender = self.gender_embedder(x)\n",
    "            else:\n",
    "                x_gender = self.gender_embedder.extract_memory_embedding(x)\n",
    "        x_corpus = None\n",
    "        if self.corpus_embedder:\n",
    "            if self.after_fc_embedded:\n",
    "                x_corpus = self.corpus_embedder(x)\n",
    "            else:\n",
    "                x_corpus = self.corpus_embedder.extract_memory_embedding(x)\n",
    "        return self.classifier(x_spect, x_speaker, x_gender, x_corpus)\n",
    "\n",
    "\n",
    "class SpeakerCCTClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None,\n",
    "                 type='self-attention',\n",
    "                 n_layers_scnd_transformer=4,\n",
    "                 n_heads_scnd_transformer=2,\n",
    "                 after_fc_embedded=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_tokens = 0\n",
    "        self.num_classes = num_classes\n",
    "        self.after_fc_embedded=after_fc_embedded\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        assert type == 'self-attention' or type == 'concatenate' or type == 'sum' or type == 'average', \\\n",
    "            f\"Union output type is set to {type} that is not specified\"\n",
    "        self.union_type = type\n",
    "\n",
    "        self.attention_pool_cct = Linear(self.embedding_dim, 1)\n",
    "        #self.attention_pool_speaker = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm_cct = LayerNorm(embedding_dim)\n",
    "\n",
    "        #self.speaker_encoder = wavencoder.models.Wav2Vec(pretrained=True)\n",
    "        #self.fc_wav = Linear(5600, embedding_dim)\n",
    "        self.lstm = LSTM(embedding_dim, embedding_dim)\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        fc_dim = embedding_dim\n",
    "\n",
    "        if self.union_type == 'concatenate':\n",
    "            fc_dim = embedding_dim * 2\n",
    "\n",
    "        if self.after_fc_embedded:\n",
    "            self.norm_spect = LayerNorm(fc_dim)\n",
    "            self.fc_spect = Linear(fc_dim, num_classes)\n",
    "            self.project_speaker = Linear(11, num_classes)\n",
    "            self.project_gender = Linear(2, num_classes)\n",
    "            self.project_corpus = Linear(5, num_classes)\n",
    "            self.fc = Linear(num_classes, num_classes)\n",
    "            if self.union_type == 'self-attention':\n",
    "                \"\"\"\n",
    "                self.self_attn = Attention(dim=embedding_dim, num_heads=4,\n",
    "                                               attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "                \"\"\"\n",
    "                self.blocks_scnd_level = ModuleList([\n",
    "                    TransformerEncoderLayer(d_model=num_classes, nhead=n_heads_scnd_transformer,\n",
    "                                            dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                            attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "                    for i in range(n_layers_scnd_transformer)])\n",
    "                self.positional_emb_scnd_level = Parameter(self.sinusoidal_embedding(3, embedding_dim),\n",
    "                                                           requires_grad=False)\n",
    "                self.norm_union = LayerNorm(num_classes)\n",
    "\n",
    "        else:\n",
    "            if self.union_type == 'self-attention':\n",
    "                \"\"\"\n",
    "                self.self_attn = Attention(dim=embedding_dim, num_heads=4,\n",
    "                                               attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "                \"\"\"\n",
    "                self.blocks_scnd_level = ModuleList([\n",
    "                    TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads_scnd_transformer,\n",
    "                                            dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                            attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "                    for i in range(n_layers_scnd_transformer)])\n",
    "                self.positional_emb_scnd_level = Parameter(self.sinusoidal_embedding(3, embedding_dim),\n",
    "                                                           requires_grad=False)\n",
    "                self.speaker_attention_pool = Linear(self.embedding_dim, 1)\n",
    "\n",
    "            self.norm_union = LayerNorm(embedding_dim)\n",
    "            self.fc = Linear(embedding_dim, num_classes)\n",
    "\n",
    "\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x_spect, x_speaker=None, x_gender=None, x_corpus=None):\n",
    "        if self.positional_emb is None and x_spect.size(1) < self.sequence_length:\n",
    "            x_spect = F.pad(x_spect, (0, 0, 0, self.n_channels - x_spect.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x_spect += self.positional_emb\n",
    "\n",
    "        x_spect = self.dropout(x_spect)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x_spect = blk(x_spect)\n",
    "        #x_spect = self.lstm(x_spect)[0]\n",
    "        x_spect = self.norm_cct(x_spect)\n",
    "        x_spect = torch.matmul(F.softmax(self.attention_pool_cct(x_spect), dim=1).transpose(-1, -2), x_spect).squeeze(\n",
    "            -2)\n",
    "\n",
    "        \"\"\"\n",
    "        #x_speaker = self.speaker_encoder(x_wav)\n",
    "        #x_speaker = x_wav\n",
    "\n",
    "        x_speaker = torch.flatten(x_wav, start_dim=1)\n",
    "        \n",
    "        x_speaker = torch.matmul(F.softmax(self.attention_pool_speaker(x_wav), dim=1).transpose(-1, -2), x_wav) \\\n",
    "            .squeeze(-2)\n",
    "        \n",
    "        x_speaker = self.fc_wav(x_speaker)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        x_spect = self.norm_spect(x_spect)\n",
    "        x_spect = self.fc_spect(x_spect)\n",
    "        \"\"\"\n",
    "        if self.after_fc_embedded:\n",
    "            x_spect = self.norm_spect(x_spect)\n",
    "            x_spect = self.fc_spect(x_spect)\n",
    "            if x_speaker is not None:\n",
    "                x_speaker = self.project_speaker(x_speaker)\n",
    "            if x_gender is not None:\n",
    "                x_gender = self.project_gender(x_gender)\n",
    "            if x_corpus is not None:\n",
    "                x_corpus = self.project_corpus(x_corpus)\n",
    "\n",
    "        else:\n",
    "            x_speaker = torch.matmul(F.softmax(self.speaker_attention_pool(x_speaker), dim=1).transpose(-1, -2), x_speaker).squeeze(-2)\n",
    "\n",
    "        if self.union_type == 'concatenate':\n",
    "            x = torch.cat((x_spect, x_speaker), 1)\n",
    "        elif self.union_type == 'sum':\n",
    "            x = torch.sum(torch.stack([x_spect, x_speaker]), dim=0)\n",
    "        elif self.union_type == 'average':\n",
    "            x = torch.mean(torch.stack([x_spect, x_speaker]), dim=0)\n",
    "\n",
    "        elif self.union_type == 'self-attention':\n",
    "\n",
    "            if x_speaker is not None and x_gender is not None and x_corpus is not None:\n",
    "                x = torch.stack((x_spect, x_speaker, x_gender, x_corpus), 1)\n",
    "            elif x_speaker is None:\n",
    "                if x_gender is not None and x_corpus is not None:\n",
    "                    x = torch.stack((x_spect, x_gender, x_corpus), 1)\n",
    "                elif x_gender is None and x_corpus is not None:\n",
    "                    x = torch.stack((x_spect, x_corpus), 1)\n",
    "                else:\n",
    "                    x = torch.stack((x_spect, x_speaker), 1)\n",
    "            elif x_gender is None:\n",
    "                if x_speaker is not None and x_corpus is not None:\n",
    "                    x = torch.stack((x_spect, x_speaker, x_corpus), 1)\n",
    "                elif x_speaker is None and x_corpus is not None:\n",
    "                    x = torch.stack((x_spect, x_corpus), 1)\n",
    "                else:\n",
    "                    x = torch.stack((x_spect, x_speaker), 1)\n",
    "            elif x_corpus is None:\n",
    "                if x_speaker is not None and x_gender is not None:\n",
    "                    x = torch.stack((x_spect, x_speaker, x_gender), 1)\n",
    "                elif x_speaker is None and x_gender is not None:\n",
    "                    x = torch.stack((x_spect, x_gender), 1)\n",
    "                else:\n",
    "                    x = torch.stack((x_spect, x_speaker), 1)\n",
    "            \"\"\"\n",
    "            x = self.self_attn(x)[:, 0, :]\n",
    "            \"\"\"\n",
    "\n",
    "            #x += self.positional_emb_scnd_level\n",
    "            for blk in self.blocks_scnd_level:\n",
    "                x = blk(x)\n",
    "            x = x[:, 0, :]\n",
    "\n",
    "        x = self.norm_union(x)\n",
    "        \"\"\"\n",
    "        #regularization\n",
    "        sum_batch_sample = 0\n",
    "        for batch_sample_index in np.arange(0, x.shape[0]):\n",
    "            sum_batch_sample += x[batch_sample_index].norm(dim=0, p=1)\n",
    "        \n",
    "        self.fc.requires_grad = False\n",
    "        \n",
    "        regularization_vector = x.shape[0] / (self.num_classes * sum_batch_sample.item())\n",
    "        for fc_weight_index in np.arange(0, self.fc.weight.shape[0]):\n",
    "            #fc_column_weights = self.fc.weight[fc_weight_index, :]\n",
    "            self.fc.weight[fc_weight_index, :] = torch.from_numpy(np.repeat(regularization_vector, self.fc.weight.shape[1]))\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "def _read_embedder_model(embedder_path, num_embedder_classes=11):\n",
    "    model_name = 'mcct_7_3x1_32_c100'\n",
    "    create_fn = model_entrypoint(model_name)\n",
    "    with set_layer_config(scriptable=None, exportable=None, no_jit=None):\n",
    "        model = create_fn(num_classes=num_embedder_classes)\n",
    "\n",
    "    if os.path.splitext(embedder_path)[-1].lower() in ('.npz', '.npy'):\n",
    "        # numpy checkpoint, try to load via model specific load_pretrained fn\n",
    "        if hasattr(model, 'load_pretrained'):\n",
    "            model.load_pretrained(embedder_path)\n",
    "        else:\n",
    "            raise NotImplementedError('Model cannot load numpy checkpoint')\n",
    "        return\n",
    "    state_dict = load_state_dict(embedder_path, False)\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    return model\n",
    "\n",
    "def _speaker_cct(pretrained, progress, pretrained_arch,\n",
    "                 num_layers, num_heads, mlp_ratio, embedding_dim, speaker=True, gender=False, corpus=False,\n",
    "                 speaker_embedder_path=None, gender_embedder_path=None,\n",
    "                 corpus_embedder_path=None, kernel_size=3, stride=None, padding=None, positional_embedding='learnable',\n",
    "                 type='self-attention', *args, **kwargs):\n",
    "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
    "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
    "    speaker_embedder = None\n",
    "    if speaker:\n",
    "        speaker_embedder = _read_embedder_model(speaker_embedder_path)\n",
    "    gender_embedder = None\n",
    "    if gender:\n",
    "        gender_embedder = _read_embedder_model(gender_embedder_path, num_embedder_classes=2)\n",
    "    corpus_embedder = None\n",
    "    if corpus:\n",
    "        corpus_embedder = _read_embedder_model(corpus_embedder_path, num_embedder_classes=5)\n",
    "    model = SpeakerCCT(speaker_embedder=speaker_embedder,\n",
    "                       gender_embedder=gender_embedder,\n",
    "                       corpus_embedder=corpus_embedder,\n",
    "                       num_layers=num_layers,\n",
    "                      num_heads=num_heads,\n",
    "                      mlp_ratio=mlp_ratio,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      type=type,\n",
    "                      *args, **kwargs)\n",
    "\n",
    "\n",
    "    if pretrained:\n",
    "        if pretrained_arch in model_urls:\n",
    "            state_dict = load_state_dict_from_url(model_urls[pretrained_arch],\n",
    "                                                  progress=progress)\n",
    "            if positional_embedding == 'learnable':\n",
    "                state_dict = pe_check(model, state_dict)\n",
    "            elif positional_embedding == 'sine':\n",
    "                state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v for k, v in state_dict.items() if\n",
    "                                k in model_dict and v.size() == model_dict[k].size()}\n",
    "            model_dict.update(state_dict)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        else:\n",
    "            raise RuntimeError(f'Variant {pretrained_arch} does not yet have pretrained weights.')\n",
    "\n",
    "    return model\n",
    "\n",
    "def speaker_cct_4(pretrained, progress, type, *args, **kwargs):\n",
    "    return _speaker_cct( pretrained=pretrained, progress=progress,\n",
    "                         num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
    "                        type=type, *args, **kwargs)\n",
    "\n",
    "def speaker_cct_7(pretrained, progress, type, *args, **kwargs):\n",
    "    return _speaker_cct( pretrained=pretrained, progress=progress,\n",
    "                         num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                         type=type, *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                         speaker_embedder_path=None,\n",
    "                        *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='self-attention',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 n_layers_scnd_transformer=1,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_gender_cct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                         speaker_embedder_path=None,\n",
    "                         gender_embedder_path=None,\n",
    "                        *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_gender_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='self-attention',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 gender_embedder_path=gender_embedder_path,\n",
    "                                 gender=True,\n",
    "                                 n_layers_scnd_transformer=2,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_corpus_cct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                         speaker_embedder_path=None,\n",
    "                         corpus_embedder_path=None,\n",
    "                        *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_corpus_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='self-attention',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 corpus_embedder_path=corpus_embedder_path,\n",
    "                                 corpus=True,\n",
    "                                 n_layers_scnd_transformer=2,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def speaker_gender_corpus_cct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                         speaker_embedder_path = None,\n",
    "                         gender_embedder_path=None,\n",
    "                         corpus_embedder_path=None,\n",
    "                        *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_gender_corpus_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='self-attention',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 gender_embedder_path=gender_embedder_path,\n",
    "                                 corpus_embedder_path=corpus_embedder_path,\n",
    "                                    gender=True, corpus=True,\n",
    "                                    *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_7_3x1_32_concatenate(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                         speaker_embedder_path = None,\n",
    "                        *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_7_3x1_32_sum(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                             speaker_embedder_path=None,\n",
    "                             *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_cct_7_3x1_32_sum', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='sum',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_7_3x1_32_average(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                                 speaker_embedder_path=None,\n",
    "                                 *args, **kwargs):\n",
    "    return speaker_cct_7(arch='speaker_cct_7_3x1_32_average', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='average',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_4_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                         speaker_embedder_path=None,\n",
    "                         *args, **kwargs):\n",
    "    return speaker_cct_4(arch='speaker_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='concatenate',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_4_3x1_32_sum(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                             speaker_embedder_path=None,\n",
    "                             *args, **kwargs):\n",
    "    return speaker_cct_4(arch='speaker_cct_7_3x1_32_sum', pretrained=pretrained, progress=progress,  pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='sum',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_cct_4_3x1_32_average(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                                 speaker_embedder_path=None,\n",
    "                                 *args, **kwargs):\n",
    "    return speaker_cct_4(arch='speaker_cct_7_3x1_32_average', pretrained=pretrained, progress=progress,  pretrained_arch=\"\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes, type='average',\n",
    "                                 speaker_embedder_path=speaker_embedder_path,\n",
    "                                 *args, **kwargs)\n"
   ],
   "metadata": {
    "id": "uy5toy5RM0Cr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483365,
     "user_tz": -120,
     "elapsed": 1554,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": 49,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-49-d72bea2fff2b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#import wavencoder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mlibrosa\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;31m#from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhub\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mload_state_dict_from_url\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/librosa/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[0;31m# And all the librosa sub-modules\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0m_cache\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcache\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 211\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    212\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbeat\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdecompose\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/librosa/core/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mconvert\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m  \u001B[0;31m# pylint: disable=wildcard-import\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0maudio\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m  \u001B[0;31m# pylint: disable=wildcard-import\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mspectrum\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m  \u001B[0;31m# pylint: disable=wildcard-import\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mpitch\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m  \u001B[0;31m# pylint: disable=wildcard-import\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/librosa/core/audio.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0maudioread\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignal\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mresampy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/signal/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mspectral\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mwavelets\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 307\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0m_peak_finding\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mwindows\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_window\u001B[0m  \u001B[0;31m# keep this one in signal namespace\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/signal/_peak_finding.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwavelets\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcwt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mricker\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstats\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mscoreatpercentile\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m from ._peak_finding_utils import (\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    387\u001B[0m \"\"\"\n\u001B[0;32m--> 388\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mstats\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    389\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mdistributions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mmorestats\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspecial\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mspecial\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlinalg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 180\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdistributions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    181\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmstats_basic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/distributions.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m                                     rv_frozen)\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0m_continuous_distns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0m_discrete_distns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    798\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    799\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 800\u001B[0;31m \u001B[0mbetaprime\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbetaprime_gen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'betaprime'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    801\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    802\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001B[0m\n\u001B[1;32m   1658\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1659\u001B[0m                 \u001B[0mdct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdistcont\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1660\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_construct_doc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdocdict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1661\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1662\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_updated_ctor_param\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001B[0m in \u001B[0;36m_construct_doc\u001B[0;34m(self, docdict, shapes_vals)\u001B[0m\n\u001B[1;32m    739\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%(shapes)s, \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    740\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 741\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdoccer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdocformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtempdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    742\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    743\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Unable to construct docstring for distribution \\\"%s\\\": %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrepr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/_lib/doccer.py\u001B[0m in \u001B[0;36mdocformat\u001B[0;34m(docstring, docdict)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m                 \u001B[0mnewlines\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindent\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0mindented\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'\\n'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewlines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mIndexError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0mindented\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdstr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SPEAKER VGGVox CCT"
   ],
   "metadata": {
    "id": "vHexoGMZNn0g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile speaker_vgg_vox_cct.py\n",
    "\n",
    "import os\n",
    "from tokenizer import Tokenizer\n",
    "from transformers_component import TransformerEncoderLayer\n",
    "from helpers import pe_check\n",
    "import torch.nn as nn\n",
    "from timm.models.helpers import load_state_dict\n",
    "from timm.models.layers import set_layer_config\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Module, ModuleList, Linear, Dropout, LayerNorm, Parameter, init, LSTM\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from timm.models.registry import register_model, model_entrypoint\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "model_urls = {\n",
    "    'cct_7_3x1_32':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar10_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar10_5000epochs.pth',\n",
    "    'cct_7_3x1_32_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_cifar100_300epochs.pth',\n",
    "    'cct_7_3x1_32_sine_c100':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_3x1_32_sine_cifar100_5000epochs.pth',\n",
    "    'cct_7_7x2_224_sine':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_7_7x2_224_flowers102.pth',\n",
    "    'cct_14_7x2_224':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth',\n",
    "    'cct_14_7x2_384':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_imagenet.pth',\n",
    "    'cct_14_7x2_384_fl':\n",
    "        'http://ix.cs.uoregon.edu/~alih/compact-transformers/checkpoints/finetuned/cct_14_7x2_384_flowers102.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class SpeakerVGGVoxCCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 n_conv_layers=1,\n",
    "                 kernel_size=7,\n",
    "                 stride=2,\n",
    "                 padding=3,\n",
    "                 pooling_kernel_size=3,\n",
    "                 pooling_stride=2,\n",
    "                 pooling_padding=1,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 n_layers_scnd_transformer=4,\n",
    "                 n_heads_scnd_transformer=2,\n",
    "                 *args, **kwargs):\n",
    "        super(SpeakerVGGVoxCCT, self).__init__()\n",
    "\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   pooling_kernel_size=pooling_kernel_size,\n",
    "                                   pooling_stride=pooling_stride,\n",
    "                                   pooling_padding=pooling_padding,\n",
    "                                   max_pool=True,\n",
    "                                   activation=nn.ReLU,\n",
    "                                   n_conv_layers=n_conv_layers,\n",
    "                                   conv_bias=False)\n",
    "\n",
    "        self.classifier = SpeakerVGGVoxCCTClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding,\n",
    "            n_layers_scnd_transformer=n_layers_scnd_transformer,\n",
    "            n_heads_scnd_transformer=n_heads_scnd_transformer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emotion = x[0]\n",
    "        x_speaker = x[1]\n",
    "        x_emotion = self.tokenizer(x_emotion)\n",
    "        return self.classifier(x_emotion, x_speaker)\n",
    "\n",
    "\n",
    "class SpeakerVGGVoxCCTClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None,\n",
    "                 n_layers_scnd_transformer=4,\n",
    "                 n_heads_scnd_transformer=2,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_tokens = 0\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        self.attention_pool_cct = Linear(self.embedding_dim, 1)\n",
    "        # self.attention_pool_speaker = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm_cct = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lstm = LSTM(embedding_dim, embedding_dim)\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.blocks_scnd_level = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads_scnd_transformer,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(n_layers_scnd_transformer)])\n",
    "        self.positional_emb_scnd_level = Parameter(self.sinusoidal_embedding(3, embedding_dim),\n",
    "                                                   requires_grad=False)\n",
    "        self.project_vgg_features = Linear(1024, embedding_dim)\n",
    "\n",
    "        self.norm_after_blocks = LayerNorm(embedding_dim)\n",
    "        self.fc = Linear(embedding_dim, num_classes)\n",
    "\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x_emotion, x_speaker):\n",
    "        if self.positional_emb is None and x_emotion.size(1) < self.sequence_length:\n",
    "            x_emotion = F.pad(x_emotion, (0, 0, 0, self.n_channels - x_emotion.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x_emotion += self.positional_emb\n",
    "\n",
    "        x_emotion = self.dropout(x_emotion)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x_emotion = blk(x_emotion)\n",
    "        # x_spect = self.lstm(x_spect)[0]\n",
    "        x_emotion = self.norm_cct(x_emotion)\n",
    "        x_emotion = torch.matmul(F.softmax(self.attention_pool_cct(x_emotion), dim=1).transpose(-1, -2),\n",
    "                                 x_emotion).squeeze(\n",
    "            -2)\n",
    "\n",
    "        x_speaker = self.project_vgg_features(x_speaker)\n",
    "\n",
    "        x = torch.stack((x_emotion, x_speaker), 1)\n",
    "\n",
    "        # x += self.positional_emb_scnd_level\n",
    "        for blk in self.blocks_scnd_level:\n",
    "            x = blk(x)\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        x = self.norm_after_blocks(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, Linear):\n",
    "            init.normal_(m.weight, std=.02)\n",
    "            if isinstance(m, Linear) and m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, LayerNorm):\n",
    "            init.constant_(m.bias, 0)\n",
    "            init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_embedding(n_channels, dim):\n",
    "        pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                                for p in range(n_channels)])\n",
    "        pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "\n",
    "def _speaker_vgg_cct(pretrained, progress, pretrained_arch,\n",
    "                     num_layers, num_heads, mlp_ratio, embedding_dim,\n",
    "                     kernel_size=3, stride=None, padding=None, positional_embedding='learnable',\n",
    "                     *args, **kwargs):\n",
    "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
    "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
    "    model = SpeakerVGGVoxCCT(num_layers=num_layers,\n",
    "                             num_heads=num_heads,\n",
    "                             mlp_ratio=mlp_ratio,\n",
    "                             embedding_dim=embedding_dim,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             *args, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        if pretrained_arch in model_urls:\n",
    "            state_dict = load_state_dict_from_url(model_urls[pretrained_arch],\n",
    "                                                  progress=progress)\n",
    "            if positional_embedding == 'learnable':\n",
    "                state_dict = pe_check(model, state_dict)\n",
    "            elif positional_embedding == 'sine':\n",
    "                state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
    "\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v for k, v in state_dict.items() if\n",
    "                          k in model_dict and v.size() == model_dict[k].size()}\n",
    "            model_dict.update(state_dict)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        else:\n",
    "            raise RuntimeError(f'Variant {pretrained_arch} does not yet have pretrained weights.')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def speaker_vgg_cct_7(pretrained, progress, *args, **kwargs):\n",
    "    return _speaker_vgg_cct(pretrained=pretrained, progress=progress,\n",
    "                            num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                            *args, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def speaker_vgg_cct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable',\n",
    "                             num_classes=10,\n",
    "                             *args, **kwargs):\n",
    "    return speaker_vgg_cct_7(arch='speaker_vgg_cct_7_3x1_32', pretrained=pretrained, progress=progress,\n",
    "                             pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                             kernel_size=3, n_conv_layers=1,\n",
    "                             img_size=img_size, positional_embedding=positional_embedding,\n",
    "                             num_classes=num_classes,\n",
    "                             n_layers_scnd_transformer=1,\n",
    "                             n_heads_scnd_transformer=1,\n",
    "                             *args, **kwargs)\n",
    "\n",
    "            model_dict = model.state_dict()\n",
    "\n",
    "            state_dict = {k: v for k, v in state_dict.items() if\n",
    "                                k in model_dict and v.size() == model_dict[k].size()}\n",
    "            model_dict.update(state_dict)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        else:\n",
    "            raise RuntimeError(f'Variant {pretrained_arch} does not yet have pretrained weights.')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def speaker_vgg_cct_7(pretrained, progress, *args, **kwargs):\n",
    "    return _speaker_vgg_cct( pretrained=pretrained, progress=progress,\n",
    "                         num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
    "                         *args, **kwargs)\n",
    "\n",
    "@register_model\n",
    "def speaker_vgg_cct_7_3x1_32(pretrained=False, progress=False, img_size=32, positional_embedding='learnable', num_classes=10,\n",
    "                        *args, **kwargs):\n",
    "    return speaker_vgg_cct_7(arch='speaker_vgg_cct_7_3x1_32', pretrained=pretrained, progress=progress,  pretrained_arch=\"cct_7_3x1_32_c100\",\n",
    "                                 kernel_size=3, n_conv_layers=1,\n",
    "                                 img_size=img_size, positional_embedding=positional_embedding,\n",
    "                                 num_classes=num_classes,\n",
    "                                 n_layers_scnd_transformer=1,\n",
    "                                 n_heads_scnd_transformer=1,\n",
    "                                 *args, **kwargs)\n",
    "     "
   ],
   "metadata": {
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483366,
     "user_tz": -120,
     "elapsed": 17,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToJd0yh8N73q",
    "outputId": "5b1041e0-618e-43fc-ed8f-ce8e51af4646"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Rpz03tFCGLAW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483366,
     "user_tz": -120,
     "elapsed": 16,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INIT"
   ],
   "metadata": {
    "id": "ajUU-Tw3c-R3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile __init__.py\n",
    "\n",
    "from speaker_vgg_vox_cct import *\n",
    "from cct import *\n",
    "\n",
    "\"\"\"\n",
    "from .cvt import *\n",
    "from .vit import *\n",
    "from .memory_cct import *\n",
    "from .memory_cvt import *\n",
    "from .parallel_memory_vit import *\n",
    "from .speaker_cct import *\n",
    "from .hybrid_cct import *\n",
    "\"\"\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cds7jSTCciaJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483366,
     "user_tz": -120,
     "elapsed": 16,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "719f99f7-a56a-4928-cf42-4a5281002928"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUSTOM DATASET"
   ],
   "metadata": {
    "id": "0JhMQm0HS7ZY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile parser_csv.py\n",
    "\n",
    "import pickle\n",
    "\n",
    "from timm.data.parsers.parser import Parser\n",
    "\n",
    "class ParserCSV(Parser):\n",
    "    def __init__(self, csv_dataset_path, class_to_idx, read_wav=False, read_vgg_features=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.samples, self.classes = self.__read_all_dataset_files(csv_dataset_path)\n",
    "\n",
    "        self.wav_data = []\n",
    "        if read_wav:\n",
    "            self.wav_data = self.__read_all_wav_files(csv_dataset_path)\n",
    "        self.vgg_features = {}\n",
    "        if read_vgg_features:\n",
    "            self.vgg_features = self.__read_vgg_vox_features()\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __read_all_dataset_files(self, csv_dataset_path):\n",
    "        dataset_csv = open(csv_dataset_path)\n",
    "        csvreader = csv.reader(dataset_csv)\n",
    "        all_dataset_files = []\n",
    "        classes_files = []\n",
    "        for idx, row in enumerate(csvreader):\n",
    "            if idx > 0:\n",
    "                all_dataset_files.append(row[0])\n",
    "                classes_files.append(row[1])\n",
    "        return all_dataset_files, classes_files\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index]\n",
    "        target = self.class_to_idx[self.classes[index]]\n",
    "        if len(self.wav_data):\n",
    "            waveform, _ = torchaudio.load(self.wav_data[index])\n",
    "            return (open(path, \"rb\"), self.wav_data[index]), target\n",
    "        if len(self.vgg_features):\n",
    "          vgg_features = self.vgg_features[os.path.basename(path).split('.')[0]]\n",
    "          return (open(path, \"rb\"), vgg_features), target\n",
    "        return open(path, \"rb\"), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __read_all_wav_files(self, csv_dataset_path):\n",
    "        dataset_csv = open(csv_dataset_path)\n",
    "        csvreader = csv.reader(dataset_csv)\n",
    "        all_wav_files = []\n",
    "        for idx, row in enumerate(csvreader):\n",
    "            if idx > 0:\n",
    "                spectrogram_path = row[0]\n",
    "                spectrogram_filename = os.path.basename(spectrogram_path)\n",
    "                original_wav_filename = self.__remove_augmentation_from_filename(spectrogram_filename)\n",
    "                root_spectrogram_path = os.path.split(os.path.split(spectrogram_path)[0])[0]\n",
    "                dataset = os.path.basename(root_spectrogram_path)\n",
    "                wav_folder = '_'.join([dataset, 'wav'])\n",
    "                wav_path = os.path.join(root_spectrogram_path, wav_folder, original_wav_filename)\n",
    "                all_wav_files.append(wav_path)\n",
    "        return all_wav_files\n",
    "      \n",
    "    def __read_vgg_vox_features(self):\n",
    "        first_spectrogram_path =self.samples[0]\n",
    "        root_path = os.path.split(os.path.split(first_spectrogram_path)[0])[0]\n",
    "        dataset = os.path.basename(root_path)\n",
    "        vgg_features_folder = '_'.join([dataset, 'vgg_vox_features'])\n",
    "        vgg_features_path = os.path.join(root_path, vgg_features_folder, \"vggvox_features.pkl\")\n",
    "       \n",
    "        with open(vgg_features_path, 'rb') as handle:\n",
    "          vgg_features = pickle.load(handle)\n",
    "        return vgg_features\n",
    "\n",
    "    def __remove_augmentation_from_filename(self, filename):\n",
    "        augmentations_methods = [\"loud\", \"noise\", \"speed\", \"pitch\", \"shift\", \"norm\", \"mask\", \"maskfreq\"]\n",
    "        name, _ = os.path.splitext(filename)\n",
    "        splitted_filename = name.split(\"_\")\n",
    "\n",
    "        last_part_filename = splitted_filename[len(splitted_filename)-1]\n",
    "        if last_part_filename in augmentations_methods:\n",
    "            '_'.join(name.split('_')[:-1])\n",
    "\n",
    "        filename_without_last_part = '.'.join([name, 'wav'])\n",
    "\n",
    "        return filename_without_last_part"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cus317fNTAlu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483366,
     "user_tz": -120,
     "elapsed": 12,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "b767efa5-617b-40e1-d363-c21d971b1ff6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile speaker_vgg_dataset.py\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import torchaudio\n",
    "from PIL import Image\n",
    "\n",
    "from timm.data.parsers import create_parser\n",
    "\n",
    "from torchaudio.models import Wav2Vec2Model\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "_ERROR_RETRY = 50\n",
    "class SpeakerVGGDataset(data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root,\n",
    "            parser=None,\n",
    "            class_map=None,\n",
    "            load_bytes=False,\n",
    "            transform=None,\n",
    "            target_transform=None,\n",
    "    ):\n",
    "        if parser is None or isinstance(parser, str):\n",
    "            parser = create_parser(parser or '', root=root, class_map=class_map)\n",
    "        self.parser = parser\n",
    "        self.load_bytes = load_bytes\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self._consecutive_errors = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input, target = self.parser[index]\n",
    "        img = input[0]\n",
    "        vgg_features = input[1]\n",
    "        try:\n",
    "            img = img.read() if self.load_bytes else Image.open(img).convert('RGB')\n",
    "        except Exception as e:\n",
    "            _logger.warning(f'Skipped sample (index {index}, file {self.parser.filename(index)}). {str(e)}')\n",
    "            self._consecutive_errors += 1\n",
    "            if self._consecutive_errors < _ERROR_RETRY:\n",
    "                return self.__getitem__((index + 1) % len(self.parser))\n",
    "            else:\n",
    "                raise e\n",
    "        self._consecutive_errors = 0\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if target is None:\n",
    "            target = -1\n",
    "        elif self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return (img, vgg_features), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parser)\n",
    "\n",
    "    def filename(self, index, basename=False, absolute=False):\n",
    "        return self.parser.filename(index, basename, absolute)\n",
    "\n",
    "    def filenames(self, basename=False, absolute=False):\n",
    "        return self.parser.filenames(basename, absolute)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v19SRshBXpBY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483367,
     "user_tz": -120,
     "elapsed": 13,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "a5821fae-e8da-43d5-a4c2-e2d99448076f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile custom_loader.py\n",
    "\n",
    "\n",
    "\"\"\" Loader Factory, Fast Collate, CUDA Prefetcher\n",
    "\n",
    "Prefetcher and Fast Collate inspired by NVIDIA APEX example at\n",
    "https://github.com/NVIDIA/apex/commit/d5e2bb4bdeedd27b1dfaf5bb2b24d6c000dee9be#diff-cf86c282ff7fba81fad27a559379d5bf\n",
    "\n",
    "Hacked together by / Copyright 2019, Ross Wightman\n",
    "\"\"\"\n",
    "import random\n",
    "from functools import partial\n",
    "from typing import Callable\n",
    "\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.data.distributed_sampler import OrderedDistributedSampler, RepeatAugSampler\n",
    "from timm.data.random_erasing import RandomErasing\n",
    "from timm.data.mixup import FastCollateMixup\n",
    "\n",
    "\n",
    "def fast_collate(batch):\n",
    "    \"\"\" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)\"\"\"\n",
    "    assert isinstance(batch[0], tuple)\n",
    "    batch_size = len(batch)\n",
    "    if isinstance(batch[0][0], tuple):\n",
    "        targets = torch.zeros(batch_size, dtype=torch.int64)\n",
    "        tensor_emotion_spect = torch.zeros((batch_size, *batch[0][0][0].shape), dtype=torch.uint8)\n",
    "        tensor_speaker_emb = torch.zeros((batch_size, *batch[0][0][1].shape), dtype=torch.float32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = batch[i][1]\n",
    "            tensor_emotion_spect[i] += torch.from_numpy(batch[i][0][0])\n",
    "            tensor_speaker_emb[i] += torch.from_numpy(batch[i][0][1])\n",
    "        return (tensor_emotion_spect, tensor_speaker_emb), targets\n",
    "\n",
    "    elif isinstance(batch[0][0], np.ndarray):\n",
    "        targets = torch.tensor([b[2] for b in batch], dtype=torch.int64)\n",
    "        assert len(targets) == batch_size\n",
    "        tensor_img = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n",
    "        for i in range(batch_size):\n",
    "            tensor_img[i] += torch.from_numpy(batch[i][0])\n",
    "        tensor_wav = torch.zeros((batch_size, *batch[0][1].shape), dtype=torch.float32)\n",
    "        for i in range(batch_size):\n",
    "            tensor_wav[i] += torch.from_numpy(batch[i][1])\n",
    "        return tensor_img, tensor_wav, targets\n",
    "\n",
    "    elif isinstance(batch[0][0], torch.Tensor):\n",
    "        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n",
    "        assert len(targets) == batch_size\n",
    "        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n",
    "        for i in range(batch_size):\n",
    "            tensor[i].copy_(batch[i][0])\n",
    "        return tensor, targets\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "\n",
    "class PrefetchLoader:\n",
    "\n",
    "    def __init__(self,\n",
    "                 loader,\n",
    "                 mean=IMAGENET_DEFAULT_MEAN,\n",
    "                 std=IMAGENET_DEFAULT_STD,\n",
    "                 fp16=False,\n",
    "                 re_prob=0.,\n",
    "                 re_mode='const',\n",
    "                 re_count=1,\n",
    "                 re_num_splits=0):\n",
    "        self.loader = loader\n",
    "        self.mean = torch.tensor([x * 255 for x in mean]).cuda().view(1, 3, 1, 1)\n",
    "        self.std = torch.tensor([x * 255 for x in std]).cuda().view(1, 3, 1, 1)\n",
    "        self.fp16 = fp16\n",
    "        if fp16:\n",
    "            self.mean = self.mean.half()\n",
    "            self.std = self.std.half()\n",
    "\n",
    "        if re_prob > 0.:\n",
    "            self.random_erasing = RandomErasing(\n",
    "                probability=re_prob, mode=re_mode, max_count=re_count, num_splits=re_num_splits)\n",
    "        else:\n",
    "            self.random_erasing = None\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        stream = torch.cuda.Stream()\n",
    "        first = True\n",
    "        for next_input, next_target in self.loader:\n",
    "            next_spect = next_input[0]\n",
    "            next_speaker_emb = next_input[1]\n",
    "            #print(next_spect.shape)\n",
    "            #print(next_speaker_emb.shape)\n",
    "            #print(next_target.shape)\n",
    "            with torch.cuda.stream(stream):\n",
    "                next_spect = next_spect.cuda(non_blocking=True)\n",
    "                next_speaker_emb = next_speaker_emb.cuda(non_blocking=True)\n",
    "                next_target = next_target.cuda(non_blocking=True)\n",
    "                if self.fp16:\n",
    "                    next_spect = next_spect.half().sub_(self.mean).div_(self.std)\n",
    "                else:\n",
    "                    next_spect = next_spect.float().sub_(self.mean).div_(self.std)\n",
    "                if self.random_erasing is not None:\n",
    "                    next_spect = self.random_erasing(next_spect)\n",
    "\n",
    "            if not first:\n",
    "                yield (input_spect, input_speaker_emb), target\n",
    "            else:\n",
    "                first = False\n",
    "\n",
    "            torch.cuda.current_stream().wait_stream(stream)\n",
    "            input_spect = next_spect\n",
    "            input_speaker_emb = next_speaker_emb\n",
    "            target = next_target\n",
    "\n",
    "        yield (input_spect, input_speaker_emb), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    @property\n",
    "    def sampler(self):\n",
    "        return self.loader.sampler\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self.loader.dataset\n",
    "\n",
    "    @property\n",
    "    def mixup_enabled(self):\n",
    "        if isinstance(self.loader.collate_fn, FastCollateMixup):\n",
    "            return self.loader.collate_fn.mixup_enabled\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @mixup_enabled.setter\n",
    "    def mixup_enabled(self, x):\n",
    "        if isinstance(self.loader.collate_fn, FastCollateMixup):\n",
    "            self.loader.collate_fn.mixup_enabled = x\n",
    "\n",
    "\n",
    "def _worker_init(worker_id, worker_seeding='all'):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    assert worker_info.id == worker_id\n",
    "    if isinstance(worker_seeding, Callable):\n",
    "        seed = worker_seeding(worker_info)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed % (2 ** 32 - 1))\n",
    "    else:\n",
    "        assert worker_seeding in ('all', 'part')\n",
    "        # random / torch seed already called in dataloader iter class w/ worker_info.seed\n",
    "        # to reproduce some old results (same seed + hparam combo), partial seeding is required (skip numpy re-seed)\n",
    "        if worker_seeding == 'all':\n",
    "            np.random.seed(worker_info.seed % (2 ** 32 - 1))\n",
    "\n",
    "\n",
    "def create_custom_loader(\n",
    "        dataset,\n",
    "        input_size,\n",
    "        batch_size,\n",
    "        is_training=False,\n",
    "        use_prefetcher=True,\n",
    "        no_aug=False,\n",
    "        re_prob=0.,\n",
    "        re_mode='const',\n",
    "        re_count=1,\n",
    "        re_split=False,\n",
    "        scale=None,\n",
    "        ratio=None,\n",
    "        hflip=0.5,\n",
    "        vflip=0.,\n",
    "        color_jitter=0.4,\n",
    "        auto_augment=None,\n",
    "        num_aug_repeats=0,\n",
    "        num_aug_splits=0,\n",
    "        interpolation='bilinear',\n",
    "        mean=IMAGENET_DEFAULT_MEAN,\n",
    "        std=IMAGENET_DEFAULT_STD,\n",
    "        num_workers=1,\n",
    "        distributed=False,\n",
    "        crop_pct=None,\n",
    "        collate_fn=None,\n",
    "        pin_memory=False,\n",
    "        fp16=False,\n",
    "        tf_preprocessing=False,\n",
    "        use_multi_epochs_loader=False,\n",
    "        persistent_workers=True,\n",
    "        worker_seeding='all',\n",
    "):\n",
    "    re_num_splits = 0\n",
    "    if re_split:\n",
    "        # apply RE to second half of batch if no aug split otherwise line up with aug split\n",
    "        re_num_splits = num_aug_splits or 2\n",
    "    dataset.transform = create_transform(\n",
    "        input_size,\n",
    "        is_training=is_training,\n",
    "        use_prefetcher=use_prefetcher,\n",
    "        no_aug=no_aug,\n",
    "        scale=scale,\n",
    "        ratio=ratio,\n",
    "        hflip=hflip,\n",
    "        vflip=vflip,\n",
    "        color_jitter=color_jitter,\n",
    "        auto_augment=auto_augment,\n",
    "        interpolation=interpolation,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        crop_pct=crop_pct,\n",
    "        tf_preprocessing=tf_preprocessing,\n",
    "        re_prob=re_prob,\n",
    "        re_mode=re_mode,\n",
    "        re_count=re_count,\n",
    "        re_num_splits=re_num_splits,\n",
    "        separate=num_aug_splits > 0,\n",
    "    )\n",
    "\n",
    "    sampler = None\n",
    "    if distributed and not isinstance(dataset, torch.utils.data.IterableDataset):\n",
    "        if is_training:\n",
    "            if num_aug_repeats:\n",
    "                sampler = RepeatAugSampler(dataset, num_repeats=num_aug_repeats)\n",
    "            else:\n",
    "                sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        else:\n",
    "            # This will add extra duplicate entries to result in equal num\n",
    "            # of samples per-process, will slightly alter validation results\n",
    "            sampler = OrderedDistributedSampler(dataset)\n",
    "    else:\n",
    "        assert num_aug_repeats == 0, \"RepeatAugment not currently supported in non-distributed or IterableDataset use\"\n",
    "\n",
    "    if collate_fn is None:\n",
    "        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate\n",
    "\n",
    "    loader_class = torch.utils.data.DataLoader\n",
    "    if use_multi_epochs_loader:\n",
    "        loader_class = MultiEpochsDataLoader\n",
    "\n",
    "    loader_args = dict(\n",
    "        batch_size=batch_size,\n",
    "        shuffle=not isinstance(dataset, torch.utils.data.IterableDataset) and sampler is None and is_training,\n",
    "        num_workers=num_workers,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=is_training,\n",
    "        worker_init_fn=partial(_worker_init, worker_seeding=worker_seeding),\n",
    "        persistent_workers=persistent_workers\n",
    "    )\n",
    "    try:\n",
    "        loader = loader_class(dataset, **loader_args)\n",
    "    except TypeError as e:\n",
    "        loader_args.pop('persistent_workers')  # only in Pytorch 1.7+\n",
    "        loader = loader_class(dataset, **loader_args)\n",
    "    if use_prefetcher:\n",
    "        prefetch_re_prob = re_prob if is_training and not no_aug else 0.\n",
    "        loader = PrefetchLoader(\n",
    "            loader,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "            fp16=fp16,\n",
    "            re_prob=prefetch_re_prob,\n",
    "            re_mode=re_mode,\n",
    "            re_count=re_count,\n",
    "            re_num_splits=re_num_splits\n",
    "        )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "class MultiEpochsDataLoader(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._DataLoader__initialized = False\n",
    "        self.batch_sampler = _RepeatSampler(self.batch_sampler)\n",
    "        self._DataLoader__initialized = True\n",
    "        self.iterator = super().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield next(self.iterator)\n",
    "\n",
    "\n",
    "class _RepeatSampler(object):\n",
    "    \"\"\" Sampler that repeats forever.\n",
    "\n",
    "    Args:\n",
    "        sampler (Sampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield from iter(self.sampler)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKYyeWcTZlvB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483367,
     "user_tz": -120,
     "elapsed": 12,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "ad2657bb-6319-4600-9ecc-a1011333ecb8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile mixup.py\n",
    "\n",
    "\"\"\" Mixup and Cutmix\n",
    "\n",
    "Papers:\n",
    "mixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n",
    "\n",
    "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899)\n",
    "\n",
    "Code Reference:\n",
    "CutMix: https://github.com/clovaai/CutMix-PyTorch\n",
    "\n",
    "Hacked together by / Copyright 2019, Ross Wightman\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def one_hot(x, num_classes, on_value=1., off_value=0., device='cuda'):\n",
    "    x = x.long().view(-1, 1)\n",
    "    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(1, x, on_value)\n",
    "\n",
    "\n",
    "def mixup_target(target, num_classes, lam=1., smoothing=0.0, device='cuda'):\n",
    "    off_value = smoothing / num_classes\n",
    "    on_value = 1. - smoothing + off_value\n",
    "    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value, device=device)\n",
    "    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value, device=device)\n",
    "    return y1 * lam + y2 * (1. - lam)\n",
    "\n",
    "def mixup_vector(vectors, lam=1.):\n",
    "    y1 = vectors\n",
    "    y2 = vectors.flip(0)\n",
    "    return y1 * lam + y2 * (1. - lam)\n",
    "\n",
    "def rand_bbox(img_shape, lam, margin=0., count=None):\n",
    "    \"\"\" Standard CutMix bounding-box\n",
    "    Generates a random square bbox based on lambda value. This impl includes\n",
    "    support for enforcing a border margin as percent of bbox dimensions.\n",
    "\n",
    "    Args:\n",
    "        img_shape (tuple): Image shape as tuple\n",
    "        lam (float): Cutmix lambda value\n",
    "        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n",
    "        count (int): Number of bbox to generate\n",
    "    \"\"\"\n",
    "    ratio = np.sqrt(1 - lam)\n",
    "    img_h, img_w = img_shape[-2:]\n",
    "    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n",
    "    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n",
    "    cy = np.random.randint(0 + margin_y, img_h - margin_y, size=count)\n",
    "    cx = np.random.randint(0 + margin_x, img_w - margin_x, size=count)\n",
    "    yl = np.clip(cy - cut_h // 2, 0, img_h)\n",
    "    yh = np.clip(cy + cut_h // 2, 0, img_h)\n",
    "    xl = np.clip(cx - cut_w // 2, 0, img_w)\n",
    "    xh = np.clip(cx + cut_w // 2, 0, img_w)\n",
    "    return yl, yh, xl, xh\n",
    "\n",
    "\n",
    "def rand_bbox_minmax(img_shape, minmax, count=None):\n",
    "    \"\"\" Min-Max CutMix bounding-box\n",
    "    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n",
    "    based on min/max percent values applied to each dimension of the input image.\n",
    "\n",
    "    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max.\n",
    "\n",
    "    Args:\n",
    "        img_shape (tuple): Image shape as tuple\n",
    "        minmax (tuple or list): Min and max bbox ratios (as percent of image size)\n",
    "        count (int): Number of bbox to generate\n",
    "    \"\"\"\n",
    "    assert len(minmax) == 2\n",
    "    img_h, img_w = img_shape[-2:]\n",
    "    cut_h = np.random.randint(int(img_h * minmax[0]), int(img_h * minmax[1]), size=count)\n",
    "    cut_w = np.random.randint(int(img_w * minmax[0]), int(img_w * minmax[1]), size=count)\n",
    "    yl = np.random.randint(0, img_h - cut_h, size=count)\n",
    "    xl = np.random.randint(0, img_w - cut_w, size=count)\n",
    "    yu = yl + cut_h\n",
    "    xu = xl + cut_w\n",
    "    return yl, yu, xl, xu\n",
    "\n",
    "\n",
    "def cutmix_bbox_and_lam(img_shape, lam, ratio_minmax=None, correct_lam=True, count=None):\n",
    "    \"\"\" Generate bbox and apply lambda correction.\n",
    "    \"\"\"\n",
    "    if ratio_minmax is not None:\n",
    "        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n",
    "    else:\n",
    "        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n",
    "    if correct_lam or ratio_minmax is not None:\n",
    "        bbox_area = (yu - yl) * (xu - xl)\n",
    "        lam = 1. - bbox_area / float(img_shape[-2] * img_shape[-1])\n",
    "    return (yl, yu, xl, xu), lam\n",
    "\n",
    "\n",
    "class Mixup:\n",
    "    \"\"\" Mixup/Cutmix that applies different params to each element or whole batch\n",
    "\n",
    "    Args:\n",
    "        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n",
    "        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n",
    "        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.\n",
    "        prob (float): probability of applying mixup or cutmix per batch or element\n",
    "        switch_prob (float): probability of switching to cutmix instead of mixup when both are active\n",
    "        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)\n",
    "        correct_lam (bool): apply lambda correction when cutmix bbox clipped by image borders\n",
    "        label_smoothing (float): apply label smoothing to the mixed target tensor\n",
    "        num_classes (int): number of classes for target\n",
    "    \"\"\"\n",
    "    def __init__(self, mixup_alpha=1., cutmix_alpha=0., cutmix_minmax=None, prob=1.0, switch_prob=0.5,\n",
    "                 mode='batch', correct_lam=True, label_smoothing=0.1, num_classes=1000):\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.cutmix_minmax = cutmix_minmax\n",
    "        if self.cutmix_minmax is not None:\n",
    "            assert len(self.cutmix_minmax) == 2\n",
    "            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n",
    "            self.cutmix_alpha = 1.0\n",
    "        self.mix_prob = prob\n",
    "        self.switch_prob = switch_prob\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.num_classes = num_classes\n",
    "        self.mode = mode\n",
    "        self.correct_lam = correct_lam  # correct lambda based on clipped area for cutmix\n",
    "        self.mixup_enabled = True  # set to false to disable mixing (intended tp be set by train loop)\n",
    "\n",
    "    def _params_per_elem(self, batch_size):\n",
    "        lam = np.ones(batch_size, dtype=np.float32)\n",
    "        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n",
    "        if self.mixup_enabled:\n",
    "            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n",
    "                use_cutmix = np.random.rand(batch_size) < self.switch_prob\n",
    "                lam_mix = np.where(\n",
    "                    use_cutmix,\n",
    "                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size),\n",
    "                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size))\n",
    "            elif self.mixup_alpha > 0.:\n",
    "                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size)\n",
    "            elif self.cutmix_alpha > 0.:\n",
    "                use_cutmix = np.ones(batch_size, dtype=np.bool)\n",
    "                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n",
    "            else:\n",
    "                assert False, \"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"\n",
    "            lam = np.where(np.random.rand(batch_size) < self.mix_prob, lam_mix.astype(np.float32), lam)\n",
    "        return lam, use_cutmix\n",
    "\n",
    "    def _params_per_batch(self):\n",
    "        lam = 1.\n",
    "        use_cutmix = False\n",
    "        if self.mixup_enabled and np.random.rand() < self.mix_prob:\n",
    "            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n",
    "                use_cutmix = np.random.rand() < self.switch_prob\n",
    "                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha) if use_cutmix else \\\n",
    "                    np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            elif self.mixup_alpha > 0.:\n",
    "                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            elif self.cutmix_alpha > 0.:\n",
    "                use_cutmix = True\n",
    "                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n",
    "            else:\n",
    "                assert False, \"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"\n",
    "            lam = float(lam_mix)\n",
    "        return lam, use_cutmix\n",
    "\n",
    "    def _mix_elem(self, x):\n",
    "        batch_size = len(x)\n",
    "        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n",
    "        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n",
    "        for i in range(batch_size):\n",
    "            j = batch_size - i - 1\n",
    "            lam = lam_batch[i]\n",
    "            if lam != 1.:\n",
    "                if use_cutmix[i]:\n",
    "                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n",
    "                    lam_batch[i] = lam\n",
    "                else:\n",
    "                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n",
    "        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n",
    "\n",
    "    def _mix_pair(self, x):\n",
    "        batch_size = len(x)\n",
    "        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n",
    "        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n",
    "        for i in range(batch_size // 2):\n",
    "            j = batch_size - i - 1\n",
    "            lam = lam_batch[i]\n",
    "            if lam != 1.:\n",
    "                if use_cutmix[i]:\n",
    "                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n",
    "                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n",
    "                    lam_batch[i] = lam\n",
    "                else:\n",
    "                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n",
    "                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n",
    "        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n",
    "        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n",
    "\n",
    "    def _mix_batch(self, x):\n",
    "        lam, use_cutmix = self._params_per_batch()\n",
    "        if lam == 1.:\n",
    "            return 1.\n",
    "        if use_cutmix:\n",
    "            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                x.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n",
    "        else:\n",
    "            x_flipped = x.flip(0).mul_(1. - lam)\n",
    "            x.mul_(lam).add_(x_flipped)\n",
    "        return lam\n",
    "\n",
    "    def __call__(self, x, target):\n",
    "        assert len(x) % 2 == 0, 'Batch size should be even when using this'\n",
    "        if self.mode == 'elem':\n",
    "            lam = self._mix_elem(x)\n",
    "        elif self.mode == 'pair':\n",
    "            lam = self._mix_pair(x)\n",
    "        else:\n",
    "            lam = self._mix_batch(x)\n",
    "        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, x.device)\n",
    "        return x, target\n",
    "\n",
    "\n",
    "class FastCollateMixup(Mixup):\n",
    "    \"\"\" Fast Collate w/ Mixup/Cutmix that applies different params to each element or whole batch\n",
    "\n",
    "    A Mixup impl that's performed while collating the batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def _mix_elem_collate(self, output, batch, half=False):\n",
    "        batch_size = len(batch)\n",
    "        num_elem = batch_size // 2 if half else batch_size\n",
    "        assert len(output) == num_elem\n",
    "        lam_batch, use_cutmix = self._params_per_elem(num_elem)\n",
    "        for i in range(num_elem):\n",
    "            j = batch_size - i - 1\n",
    "            lam = lam_batch[i]\n",
    "            if isinstance(batch[i][0], tuple):\n",
    "                mixed = batch[i][0][0]\n",
    "            else:\n",
    "                mixed = batch[i][0]\n",
    "            if lam != 1.:\n",
    "                if use_cutmix[i]:\n",
    "                    if not half:\n",
    "                        mixed = mixed.copy()\n",
    "                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                        output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "                    if isinstance(batch[i][0], tuple):\n",
    "                        mixed[:, yl:yh, xl:xh] = batch[j][0][0][:, yl:yh, xl:xh]\n",
    "                    else:\n",
    "                        mixed[:, yl:yh, xl:xh] = batch[j][0][:, yl:yh, xl:xh]\n",
    "                    lam_batch[i] = lam\n",
    "                else:\n",
    "                    if isinstance(batch[i][0], tuple):\n",
    "                        mixed = mixed.astype(np.float32) * lam + batch[j][0][0].astype(np.float32) * (1 - lam)\n",
    "                    else:\n",
    "                        mixed = mixed.astype(np.float32) * lam + batch[j][0].astype(np.float32) * (1 - lam)\n",
    "                    np.rint(mixed, out=mixed)\n",
    "            output[i] += torch.from_numpy(mixed.astype(np.uint8))\n",
    "        if half:\n",
    "            lam_batch = np.concatenate((lam_batch, np.ones(num_elem)))\n",
    "        return torch.tensor(lam_batch).unsqueeze(1)\n",
    "\n",
    "    def _mix_pair_collate(self, output, batch):\n",
    "        batch_size = len(batch)\n",
    "        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n",
    "        for i in range(batch_size // 2):\n",
    "            j = batch_size - i - 1\n",
    "            lam = lam_batch[i]\n",
    "            if isinstance(batch[i][0], tuple):\n",
    "                mixed_i = batch[i][0][0]\n",
    "                mixed_j = batch[j][0][0]\n",
    "            else:\n",
    "                mixed_i = batch[i][0]\n",
    "                mixed_j = batch[j][0]\n",
    "            assert 0 <= lam <= 1.0\n",
    "            if lam < 1.:\n",
    "                if use_cutmix[i]:\n",
    "                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                        output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "                    patch_i = mixed_i[:, yl:yh, xl:xh].copy()\n",
    "                    mixed_i[:, yl:yh, xl:xh] = mixed_j[:, yl:yh, xl:xh]\n",
    "                    mixed_j[:, yl:yh, xl:xh] = patch_i\n",
    "                    lam_batch[i] = lam\n",
    "                else:\n",
    "                    mixed_temp = mixed_i.astype(np.float32) * lam + mixed_j.astype(np.float32) * (1 - lam)\n",
    "                    mixed_j = mixed_j.astype(np.float32) * lam + mixed_i.astype(np.float32) * (1 - lam)\n",
    "                    mixed_i = mixed_temp\n",
    "                    np.rint(mixed_j, out=mixed_j)\n",
    "                    np.rint(mixed_i, out=mixed_i)\n",
    "            output[i] += torch.from_numpy(mixed_i.astype(np.uint8))\n",
    "            output[j] += torch.from_numpy(mixed_j.astype(np.uint8))\n",
    "        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n",
    "        return torch.tensor(lam_batch).unsqueeze(1)\n",
    "\n",
    "    def _mix_batch_collate(self, output, batch):\n",
    "        batch_size = len(batch)\n",
    "        lam, use_cutmix = self._params_per_batch()\n",
    "        if use_cutmix:\n",
    "            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n",
    "                output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n",
    "        for i in range(batch_size):\n",
    "            j = batch_size - i - 1\n",
    "            if isinstance(batch[i][0], tuple):\n",
    "                mixed = batch[i][0][0]\n",
    "            else:\n",
    "                mixed = batch[i][0]\n",
    "            if lam != 1.:\n",
    "                if use_cutmix:\n",
    "                    mixed = mixed.copy()  # don't want to modify the original while iterating\n",
    "                    if isinstance(batch[i][0], tuple):\n",
    "                        mixed[:, yl:yh, xl:xh] = batch[j][0][0][:, yl:yh, xl:xh]\n",
    "                    else:\n",
    "                        mixed[:, yl:yh, xl:xh] = batch[j][0][:, yl:yh, xl:xh]\n",
    "                else:\n",
    "                    if isinstance(batch[i][0], tuple):\n",
    "                        mixed = mixed.astype(np.float32) * lam + batch[j][0][0].astype(np.float32) * (1 - lam)\n",
    "                    else:\n",
    "                        mixed = mixed.astype(np.float32) * lam + batch[j][0].astype(np.float32) * (1 - lam)\n",
    "                    np.rint(mixed, out=mixed)\n",
    "            output[i] += torch.from_numpy(mixed.astype(np.uint8))\n",
    "        return lam\n",
    "\n",
    "    def __call__(self, batch, _=None):\n",
    "        batch_size = len(batch)\n",
    "        assert batch_size % 2 == 0, 'Batch size should be even when using this'\n",
    "        half = 'half' in self.mode\n",
    "        if half:\n",
    "            batch_size //= 2\n",
    "\n",
    "        if isinstance(batch[0][0], tuple):\n",
    "            image_shape = batch[0][0][0].shape\n",
    "        else:\n",
    "            image_shape = batch[0][0].shape\n",
    "\n",
    "        output = torch.zeros((batch_size, *image_shape), dtype=torch.uint8)\n",
    "        if self.mode == 'elem' or self.mode == 'half':\n",
    "            lam = self._mix_elem_collate(output, batch, half=half)\n",
    "        elif self.mode == 'pair':\n",
    "            lam = self._mix_pair_collate(output, batch)\n",
    "        else:\n",
    "            lam = self._mix_batch_collate(output, batch)\n",
    "        target = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n",
    "        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, device='cpu')\n",
    "        target = target[:batch_size]\n",
    "        if isinstance(batch[0][0], tuple):\n",
    "            other_features = torch.FloatTensor(np.array([b[0][1] for b in batch]))\n",
    "            other_features = mixup_vector(other_features, lam)\n",
    "            return (output, other_features), target\n",
    "        else:\n",
    "            return output, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UTILS"
   ],
   "metadata": {
    "id": "mTisugTqXQra"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "def update_graph(train_loss_history, val_loss_history, val_acc_history, path):\n",
    "    \"\"\"\n",
    "    Method that update graphs with the trend of loss and accuracy on the train and validation data\n",
    "    :param train_loss_history, val_loss_history: list that contains loss value for train and validation detected\n",
    "                                                 at each epoch\n",
    "           train_acc_history, val_loss_hystory: list that contains mean accuracy for train and validation\n",
    "                                                detected at each epoch\n",
    "           path: path of the directory where to save the graphs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    losses_img_file = os.path.join(path, \"training_losses.png\")\n",
    "    acc_img_file = os.path.join(path, \"training_accuracy.png\")\n",
    "    epochs = np.arange(1, len(train_loss_history) + 1)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Plot Training/Validation Losses\")\n",
    "    plt.ylim(0, max(max(train_loss_history), max(val_loss_history)))\n",
    "    plt.plot(epochs, train_loss_history, label=\"average train loss\")\n",
    "    plt.plot(epochs, val_loss_history, label=\"average validation loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(losses_img_file)\n",
    "    plt.close()\n",
    "    plt.title(\"Plot Validation Accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy(%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    # plt.plot(epochs, train_acc_history, label=\"average train accuracy\")\n",
    "    plt.plot(epochs, val_acc_history, label=\"average validation accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(acc_img_file)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def read_history_from_csv(path):\n",
    "    csv_path = os.path.join(path, \"summary.csv\")\n",
    "    train_loss_history, val_loss_history, val_acc_history = [], [], []\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"ERROR: not find csv in path \", csv_path)\n",
    "        return\n",
    "    with open(csv_path, mode='r') as file:\n",
    "        for idx, line in enumerate(csv.reader(file)):\n",
    "            if line[0] != \"epoch\":\n",
    "                train_loss_history.append(float(line[1]))\n",
    "                val_loss_history.append(float(line[2]))\n",
    "                val_acc_history.append(float(line[3]))\n",
    "    return train_loss_history, val_loss_history, val_acc_history\n",
    "\n",
    "\n",
    "def save_confusion_matrix(cm, labels, fname):\n",
    "    df_cm = pd.DataFrame(cm, index=[str(i) for i in labels],\n",
    "                         columns=[str(i) for i in labels])\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sn.heatmap(df_cm, annot=True, cmap=\"Blues\")\n",
    "    plt.savefig(fname, dpi=240)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_performance(performance, path):\n",
    "    with open(path, 'w') as outfile:\n",
    "        yaml.dump(performance, outfile, default_flow_style=False)\n",
    "\n",
    "\n",
    "def get_emotion(filename):\n",
    "    head, tail = os.path.split(filename)\n",
    "    tail_split = tail.split(\"_\")\n",
    "    return tail_split[0]\n",
    "\n",
    "\n",
    "def get_class_to_idx(dataset, num_classes):\n",
    "    class_to_idx = {}\n",
    "    if num_classes == \"3_classes\":\n",
    "        class_to_idx = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
    "    elif num_classes == \"4_classes\" or num_classes == \"4_classes_excited\":\n",
    "        class_to_idx = {'anger': 0, 'happy': 1, 'neutral': 2, \"sad\": 3}\n",
    "    elif num_classes == \"all_classes\":\n",
    "        if dataset == \"EMODB\":\n",
    "            class_to_idx = {'happy': 0, 'sad': 1, 'anger': 2,\n",
    "                            'disgust': 3, 'fearful': 4, 'neutral': 5}\n",
    "        elif dataset == \"DEMOS\":\n",
    "            class_to_idx = {'happy': 0, 'sad': 1, 'anger': 2,\n",
    "                            'disgust': 3, 'fearful': 4, 'surprised': 5}\n",
    "        elif dataset == \"IEMOCAP\":\n",
    "            class_to_idx = {'happy': 0, 'sad': 1, 'anger': 2,\n",
    "                            'disgust': 3, 'fearful': 4, 'surprised': 6, 'neutral': 5,\n",
    "                            'excited': 7, 'frustration': 8}\n",
    "        else:\n",
    "            class_to_idx = {'happy': 0, 'sad': 1, 'anger': 2,\n",
    "                            'disgust': 3, 'fearful': 4, 'neutral': 5, 'surprised': 6}\n",
    "    return class_to_idx\n",
    "\n",
    "\n",
    "def get_3_class_to_idx():\n",
    "    class_to_idx = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
    "    return class_to_idx\n",
    "\n",
    "def get_speaker_class_to_idx():\n",
    "    class_to_idx = {'demos-01': 0, 'demos-02': 1, 'demos-03': 2, 'demos-04': 3,\n",
    "                    'demos-05': 4, 'emovo-01': 5, 'emovo-02': 6, 'emovo-03': 7, 'emovo-04': 8,\n",
    "                    'emovo-05': 9, 'emovo-06': 10}\n",
    "    return class_to_idx\n",
    "\n",
    "def get_gender_class_to_idx():\n",
    "    class_to_idx = {'F': 0, 'M': 1}\n",
    "    return class_to_idx\n",
    "\n",
    "def get_corpus_class_to_idx():\n",
    "    class_to_idx = {\"emovo\": 0, \"iemocap\": 1, \"ravdess\": 2, \"savee\": 3, \"emodb\": 4}\n",
    "    return class_to_idx\n",
    "\n",
    "def get_class_labels(train_dataset_path):\n",
    "    classes_label = []\n",
    "    for root, subdirs, files in os.walk(train_dataset_path):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[-1].lower() == \".csv\":\n",
    "                dataset_csv = open(os.path.join(root, file))\n",
    "                csvreader = csv.reader(dataset_csv)\n",
    "                for idx, row in enumerate(csvreader):\n",
    "                    if idx > 0:\n",
    "                        current_class = row[1]\n",
    "                        if current_class not in classes_label:\n",
    "                            classes_label.append(current_class)\n",
    "    return classes_label\n",
    "\n",
    "\n",
    "def check_read_wav_from_model_name(model_name):\n",
    "    first_part_name = model_name.split('_')[0]\n",
    "    if first_part_name == 'speaker':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_dataset_name_from_path(path):\n",
    "    all_datasets = [\"EMOVO\", \"IEMOCAP\", \"RAVDESS\", \"SAVEE\", \"EMODB\", \"DEMOS\", \"TESS\"]\n",
    "    path_tails = path.split('/')\n",
    "    for dataset in all_datasets:\n",
    "        if dataset in path_tails:\n",
    "            return dataset\n",
    "    return None\n",
    "\n",
    "def get_classes_name_from_path(path):\n",
    "    all_classes_name = [\"3_classes\", \"3_classes_excited\", \"4_classes\", \"4_classes_excited\", \"all_classes\",\n",
    "                        \"all_classes_excited\"]\n",
    "    path_tails = path.split('/')\n",
    "    for classes_name in all_classes_name:\n",
    "        if classes_name in path_tails:\n",
    "            return classes_name\n",
    "    return None\n",
    "\n",
    "def get_aug_type_from_path(path):\n",
    "    all_aug = [\"no_aug\", \"balanced_aug\", \"time_aug\", \"freq_aug\", \"time_freq_aud\"]\n",
    "    path_tails = path.split('/')\n",
    "    for aug in all_aug:\n",
    "        if aug in path_tails:\n",
    "            return aug\n",
    "    return None\n",
    "\n",
    "def get_exp_type_from_path(path):\n",
    "    all_exp = [\"within-corpus\", \"IEMOCAP_train\", \"cross-corpus\"]\n",
    "    path_tails = path.split('/')\n",
    "    for exp in all_exp:\n",
    "        if exp in path_tails:\n",
    "            return exp\n",
    "    return None\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nyqQI9JXSXJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483367,
     "user_tz": -120,
     "elapsed": 11,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "bcd48381-ac1a-4192-99b1-a5fb0eff7116"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING WITHIN CORPUS"
   ],
   "metadata": {
    "id": "NGArX-uWRDch"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TRAIN**"
   ],
   "metadata": {
    "id": "V2OVKp3RRkSY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile train.py\n",
    "#!/usr/bin/env python3\n",
    "# This is a slightly modified version of timm's training script\n",
    "\"\"\" ImageNet Training Script\n",
    "\n",
    "This is intended to be a lean and easily modifiable ImageNet training script that reproduces ImageNet\n",
    "training results with some of the latest networks and training techniques. It favours canonical PyTorch\n",
    "and standard Python style over trying to be able to 'do it all.' That said, it offers quite a few speed\n",
    "and training result improvements over the usual PyTorch example scripts. Repurpose as you see fit.\n",
    "\n",
    "This script was started from an early version of the PyTorch ImageNet example\n",
    "(https://github.com/pytorch/examples/tree/master/imagenet)\n",
    "\n",
    "NVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n",
    "(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n",
    "\"\"\"\n",
    "import argparse\n",
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "from hybrid_dataset import HybridDataset\n",
    "from speaker_dataset import SpeakerDataset\n",
    "from utils import update_graph, read_history_from_csv\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "from torch.nn.parallel import DistributedDataParallel as NativeDDP\n",
    "\n",
    "from timm.data import create_loader, resolve_data_config, Mixup, FastCollateMixup, AugMixDataset, \\\n",
    "    ImageDataset\n",
    "\n",
    "from timm.models import create_model, safe_model_name, resume_checkpoint, load_checkpoint, \\\n",
    "    convert_splitbn_model, model_parameters\n",
    "from timm.utils import *\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.utils import ApexScaler, NativeScaler\n",
    "from utils import *\n",
    "from src import *\n",
    "from speaker_vgg_dataset import SpeakerVGGDataset\n",
    "from parser_csv import ParserCSV\n",
    "from custom_loader import create_custom_loader\n",
    "from checkpoint import CheckpointSaver as Saver\n",
    "from mixup import FastCollateMixup as CustomFastCollateMixup\n",
    "\n",
    "try:\n",
    "\n",
    "    from apex import amp\n",
    "    from apex.parallel import DistributedDataParallel as ApexDDP\n",
    "    from apex.parallel import convert_syncbn_model\n",
    "\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    has_apex = False\n",
    "\n",
    "has_native_amp = False\n",
    "try:\n",
    "    if getattr(torch.cuda.amp, 'autocast') is not None:\n",
    "        has_native_amp = True\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    has_wandb = True\n",
    "except ImportError:\n",
    "    has_wandb = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "_logger = logging.getLogger('train')\n",
    "\n",
    "# The first arg parser parses out only the --config argument, this argument is used to\n",
    "# load a yaml file containing key-values that override the defaults for the main parser below\n",
    "config_parser = parser = argparse.ArgumentParser(description='Training Config', add_help=False)\n",
    "parser.add_argument('-c', '--config', default='', type=str, metavar='FILE',\n",
    "                    help='YAML config file specifying default arguments')\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "\n",
    "# Dataset / Model parameters\n",
    "parser.add_argument('data_train_dir', metavar='DIR',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--data_eval_dir', metavar='DIR',\n",
    "                    help='path to validation')\n",
    "parser.add_argument('--root_output_dir', default=\"./output/train\", metavar='DIR',\n",
    "                    help='root output dir')\n",
    "\n",
    "parser.add_argument('--save_graphs', action='store_true', default=False,\n",
    "                    help='define if save graph of training/validation losses and accuracies')\n",
    "\n",
    "parser.add_argument('--dataset', '-d', metavar='NAME', default='',\n",
    "                    help='dataset type (default: ImageFolder/ImageTar if empty)')\n",
    "parser.add_argument('--train-split', metavar='NAME', default='train',\n",
    "                    help='dataset train split (default: train)')\n",
    "parser.add_argument('--val-split', metavar='NAME', default='validation',\n",
    "                    help='dataset validation split (default: validation)')\n",
    "parser.add_argument('--model', default='resnet101', type=str, metavar='MODEL',\n",
    "                    help='Name of model to train (default: \"countception\"')\n",
    "parser.add_argument('--pretrained', action='store_true', default=False,\n",
    "                    help='Start with pretrained version of specified network (if avail)')\n",
    "parser.add_argument('--initial-checkpoint', default='', type=str, metavar='PATH',\n",
    "                    help='Initialize model from this checkpoint (default: none)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='Resume full model and optimizer state from checkpoint (default: none)')\n",
    "parser.add_argument('--no-resume-opt', action='store_true', default=False,\n",
    "                    help='prevent resume of optimizer state when resuming model')\n",
    "parser.add_argument('--num-classes', type=int, default=None, metavar='N',\n",
    "                    help='number of label classes (Model default if None)')\n",
    "parser.add_argument('--gp', default=None, type=str, metavar='POOL',\n",
    "                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\n",
    "parser.add_argument('--img-size', type=int, default=None, metavar='N',\n",
    "                    help='Image patch size (default: None => model default)')\n",
    "parser.add_argument('--input-size', default=None, nargs=3, type=int,\n",
    "                    metavar='N N N',\n",
    "                    help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\n",
    "parser.add_argument('--crop-pct', default=None, type=float,\n",
    "                    metavar='N', help='Input image center crop percent (for validation only)')\n",
    "parser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n",
    "                    help='Override mean pixel value of dataset')\n",
    "parser.add_argument('--std', type=float, nargs='+', default=None, metavar='STD',\n",
    "                    help='Override std deviation of of dataset')\n",
    "parser.add_argument('--interpolation', default='', type=str, metavar='NAME',\n",
    "                    help='Image resize interpolation type (overrides model)')\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('-vb', '--validation-batch-size-multiplier', type=int, default=1, metavar='N',\n",
    "                    help='ratio of validation batch size to training batch size (default: 01)')\n",
    "\n",
    "# Optimizer parameters\n",
    "parser.add_argument('--opt', default='sgd', type=str, metavar='OPTIMIZER',\n",
    "                    help='Optimizer (default: \"sgd\"')\n",
    "parser.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',\n",
    "                    help='Optimizer Epsilon (default: None, use opt default)')\n",
    "parser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                    help='Optimizer Betas (default: None, use opt default)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='Optimizer momentum (default: 0.9)')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.0001,\n",
    "                    help='weight decay (default: 0.0001)')\n",
    "parser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
    "                    help='Clip gradient norm (default: None, no clipping)')\n",
    "parser.add_argument('--clip-mode', type=str, default='norm',\n",
    "                    help='Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")')\n",
    "\n",
    "# Learning rate schedule parameters\n",
    "parser.add_argument('--sched', default='step', type=str, metavar='SCHEDULER',\n",
    "                    help='LR scheduler (default: \"step\"')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
    "                    help='learning rate noise on/off epoch percentages')\n",
    "parser.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
    "                    help='learning rate noise limit percent (default: 0.67)')\n",
    "parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
    "                    help='learning rate noise std-dev (default: 01.0)')\n",
    "parser.add_argument('--lr-cycle-mul', type=float, default=1.0, metavar='MULT',\n",
    "                    help='learning rate cycle len multiplier (default: 01.0)')\n",
    "parser.add_argument('--lr-cycle-limit', type=int, default=1, metavar='N',\n",
    "                    help='learning rate cycle limit')\n",
    "parser.add_argument('--warmup-lr', type=float, default=0.0001, metavar='LR',\n",
    "                    help='warmup learning rate (default: 0.0001)')\n",
    "parser.add_argument('--min-lr', type=float, default=1e-5, metavar='LR',\n",
    "                    help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n",
    "parser.add_argument('--epochs', type=int, default=200, metavar='N',\n",
    "                    help='number of epochs to train (default: 2)')\n",
    "parser.add_argument('--epoch-repeats', type=float, default=0., metavar='N',\n",
    "                    help='epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).')\n",
    "parser.add_argument('--start-epoch', default=None, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--decay-epochs', type=float, default=30, metavar='N',\n",
    "                    help='epoch interval to decay LR')\n",
    "parser.add_argument('--warmup-epochs', type=int, default=3, metavar='N',\n",
    "                    help='epochs to warmup LR, if scheduler supports')\n",
    "parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='N',\n",
    "                    help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
    "parser.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
    "                    help='patience epochs for Plateau LR scheduler (default: 10')\n",
    "parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
    "                    help='LR decay rate (default: 0.01)')\n",
    "\n",
    "# Augmentation & regularization parameters\n",
    "parser.add_argument('--no-aug', action='store_true', default=False,\n",
    "                    help='Disable all training augmentation, override other train aug args')\n",
    "parser.add_argument('--scale', type=float, nargs='+', default=[0.08, 1.0], metavar='PCT',\n",
    "                    help='Random resize scale (default: 0.08 01.0)')\n",
    "parser.add_argument('--ratio', type=float, nargs='+', default=[3. / 4., 4. / 3.], metavar='RATIO',\n",
    "                    help='Random resize aspect ratio (default: 0.75 01.33)')\n",
    "parser.add_argument('--hflip', type=float, default=0.5,\n",
    "                    help='Horizontal flip training aug probability')\n",
    "parser.add_argument('--vflip', type=float, default=0.,\n",
    "                    help='Vertical flip training aug probability')\n",
    "parser.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n",
    "                    help='Color jitter factor (default: 0.4)')\n",
    "parser.add_argument('--aa', type=str, default=None, metavar='NAME',\n",
    "                    help='Use AutoAugment policy. \"v0\" or \"original\". (default: None)'),\n",
    "parser.add_argument('--aug-splits', type=int, default=0,\n",
    "                    help='Number of augmentation splits (default: 0, valid: 0 or >=2)')\n",
    "parser.add_argument('--jsd', action='store_true', default=False,\n",
    "                    help='Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.')\n",
    "parser.add_argument('--reprob', type=float, default=0., metavar='PCT',\n",
    "                    help='Random erase prob (default: 0.)')\n",
    "parser.add_argument('--remode', type=str, default='const',\n",
    "                    help='Random erase mode (default: \"const\")')\n",
    "parser.add_argument('--recount', type=int, default=1,\n",
    "                    help='Random erase count (default: 01)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False,\n",
    "                    help='Do not random erase first (clean) augmentation split')\n",
    "parser.add_argument('--mixup', type=float, default=0.0,\n",
    "                    help='mixup alpha, mixup enabled if > 0. (default: 0.)')\n",
    "parser.add_argument('--cutmix', type=float, default=0.0,\n",
    "                    help='cutmix alpha, cutmix enabled if > 0. (default: 0.)')\n",
    "parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
    "                    help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "parser.add_argument('--mixup-prob', type=float, default=1.0,\n",
    "                    help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
    "                    help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "parser.add_argument('--mixup-mode', type=str, default='batch',\n",
    "                    help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "parser.add_argument('--mixup-off-epoch', default=0, type=int, metavar='N',\n",
    "                    help='Turn off mixup after this epoch, disabled if 0 (default: 0)')\n",
    "parser.add_argument('--smoothing', type=float, default=0.1,\n",
    "                    help='Label smoothing (default: 0.01)')\n",
    "parser.add_argument('--train-interpolation', type=str, default='random',\n",
    "                    help='Training interpolation (random, bilinear, bicubic default: \"random\")')\n",
    "parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop-connect', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop connect rate, DEPRECATED, use drop-path (default: None)')\n",
    "parser.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop path rate (default: None)')\n",
    "parser.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop block rate (default: None)')\n",
    "\n",
    "# Batch norm parameters (only works with gen_efficientnet based models currently)\n",
    "parser.add_argument('--bn-tf', action='store_true', default=False,\n",
    "                    help='Use Tensorflow BatchNorm defaults for models that support it (default: False)')\n",
    "parser.add_argument('--bn-momentum', type=float, default=None,\n",
    "                    help='BatchNorm momentum override (if not None)')\n",
    "parser.add_argument('--bn-eps', type=float, default=None,\n",
    "                    help='BatchNorm epsilon override (if not None)')\n",
    "parser.add_argument('--sync-bn', action='store_true',\n",
    "                    help='Enable NVIDIA Apex or Torch synchronized BatchNorm.')\n",
    "parser.add_argument('--dist-bn', type=str, default='',\n",
    "                    help='Distribute BatchNorm stats between nodes after each epoch (\"broadcast\", \"reduce\", or \"\")')\n",
    "parser.add_argument('--split-bn', action='store_true',\n",
    "                    help='Enable separate BN layers per augmentation split.')\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--recovery-interval', type=int, default=0, metavar='N',\n",
    "                    help='how many batches to wait before writing recovery checkpoint')\n",
    "parser.add_argument('--checkpoint-hist', type=int, default=10, metavar='N',\n",
    "                    help='number of checkpoints to keep (default: 10)')\n",
    "parser.add_argument('-j', '--workers', type=int, default=4, metavar='N',\n",
    "                    help='how many training processes to use (default: 01)')\n",
    "parser.add_argument('--save-images', action='store_true', default=False,\n",
    "                    help='save images of input bathes every log interval for debugging')\n",
    "parser.add_argument('--amp', action='store_true', default=False,\n",
    "                    help='use NVIDIA Apex AMP or Native AMP for mixed precision training')\n",
    "parser.add_argument('--apex-amp', action='store_true', default=False,\n",
    "                    help='Use NVIDIA Apex AMP mixed precision')\n",
    "parser.add_argument('--native-amp', action='store_true', default=False,\n",
    "                    help='Use Native Torch AMP mixed precision')\n",
    "parser.add_argument('--channels-last', action='store_true', default=False,\n",
    "                    help='Use channels_last memory layout')\n",
    "parser.add_argument('--pin-mem', action='store_true', default=False,\n",
    "                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--no-prefetcher', action='store_true', default=False,\n",
    "                    help='disable fast prefetcher')\n",
    "parser.add_argument('--experiment', default='', type=str, metavar='NAME',\n",
    "                    help='name of train experiment, name of sub-folder for output')\n",
    "parser.add_argument('--eval-metric', default='top1', type=str, metavar='EVAL_METRIC',\n",
    "                    help='Best metric (default: \"top1\"')\n",
    "parser.add_argument('--tta', type=int, default=0, metavar='N',\n",
    "                    help='Test/inference time augmentation (oversampling) factor. 0=None (default: 0)')\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text\n",
    "\n",
    "\n",
    "def train(train_csv, eval_csv, output_dir, args, class_to_idx):\n",
    "    if has_wandb:\n",
    "        wandb.init(project=args.experiment, config=args)\n",
    "    else:\n",
    "        _logger.warning(\"You've requested to log metrics to wandb but package not found. \"\n",
    "                        \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    args.prefetcher = not args.no_prefetcher\n",
    "    if 'WORLD_SIZE' in os.environ:\n",
    "        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n",
    "    args.device = 'cuda:0'\n",
    "    args.world_size = 1\n",
    "\n",
    "    _logger.info('Training with a single process on 01 GPUs.')\n",
    "\n",
    "    # resolve AMP arguments based on PyTorch / Apex availability\n",
    "    use_amp = None\n",
    "    if args.amp:\n",
    "        # `--amp` chooses native amp before apex (APEX ver not actively maintained)\n",
    "        if has_native_amp:\n",
    "            args.native_amp = True\n",
    "        elif has_apex:\n",
    "            args.apex_amp = True\n",
    "    if args.apex_amp and has_apex:\n",
    "        use_amp = 'apex'\n",
    "    elif args.native_amp and has_native_amp:\n",
    "        use_amp = 'native'\n",
    "    elif args.apex_amp or args.native_amp:\n",
    "        _logger.warning(\"Neither APEX or native Torch AMP is available, using float32. \"\n",
    "                        \"Install NVIDA apex or upgrade to PyTorch 01.6\")\n",
    "\n",
    "    random_seed(args.seed, 0)\n",
    "\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_connect_rate=args.drop_connect,  # DEPRECATED, use drop_path\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_tf=args.bn_tf,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        checkpoint_path=args.initial_checkpoint)\n",
    "\n",
    "    _logger.info(\n",
    "        f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=True)\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 01 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = create_optimizer_v2(model, **optimizer_kwargs(cfg=args))\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        amp_autocast = torch.cuda.amp.autocast\n",
    "        loss_scaler = NativeScaler()\n",
    "        _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model, args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=args.local_rank == 0)\n",
    "\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    lr_scheduler, num_epochs = create_scheduler(args, optimizer)\n",
    "    start_epoch = 0\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        lr_scheduler.step(start_epoch)\n",
    "\n",
    "    _logger.info('Scheduled epochs: {}'.format(num_epochs))\n",
    "\n",
    "    # read_wav = check_read_wav_from_model_name(args.model)\n",
    "    first_part_model_name = model_name.split('_')[0]\n",
    "    second_part_model_name = model_name.split('_')[1]\n",
    "\n",
    "    read_vgg_features = False\n",
    "\n",
    "    if first_part_model_name == \"speaker\" and second_part_model_name == \"vgg\":\n",
    "        read_vgg_features = True\n",
    "        dataset_train = SpeakerVGGDataset(str(train_csv),\n",
    "                                          parser=ParserCSV(train_csv, class_to_idx=class_to_idx,\n",
    "                                                           read_vgg_features=True))\n",
    "        dataset_eval = SpeakerVGGDataset(str(eval_csv),\n",
    "                                         parser=ParserCSV(eval_csv, class_to_idx=class_to_idx, read_vgg_features=True))\n",
    "    else:\n",
    "        dataset_train = ImageDataset(str(train_csv), parser=ParserCSV(train_csv, class_to_idx=class_to_idx))\n",
    "        dataset_eval = ImageDataset(str(eval_csv), parser=ParserCSV(eval_csv, class_to_idx=class_to_idx))\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "\n",
    "\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing, num_classes=args.num_classes)\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            #collate_fn = FastCollateMixup(**mixup_args)\n",
    "            collate_fn = CustomFastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeline\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "\n",
    "    if read_vgg_features:\n",
    "        loader_train = create_custom_loader(\n",
    "            dataset_train,\n",
    "            input_size=data_config['input_size'],\n",
    "            batch_size=args.batch_size,\n",
    "            is_training=True,\n",
    "            use_prefetcher=args.prefetcher,\n",
    "            no_aug=args.no_aug,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            re_split=args.resplit,\n",
    "            scale=args.scale,\n",
    "            ratio=args.ratio,\n",
    "            hflip=args.hflip,\n",
    "            vflip=args.vflip,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            num_aug_splits=num_aug_splits,\n",
    "            interpolation=train_interpolation,\n",
    "            mean=data_config['mean'],\n",
    "            std=data_config['std'],\n",
    "            num_workers=args.workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=args.pin_mem,\n",
    "        )\n",
    "\n",
    "        loader_eval = create_custom_loader(\n",
    "            dataset_eval,\n",
    "            input_size=data_config['input_size'],\n",
    "            batch_size=args.validation_batch_size_multiplier * args.batch_size,\n",
    "            is_training=False,\n",
    "            use_prefetcher=args.prefetcher,\n",
    "            interpolation=data_config['interpolation'],\n",
    "            mean=data_config['mean'],\n",
    "            std=data_config['std'],\n",
    "            num_workers=args.workers,\n",
    "            crop_pct=data_config['crop_pct'],\n",
    "            pin_memory=args.pin_mem,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        loader_train = create_loader(\n",
    "            dataset_train,\n",
    "            input_size=data_config['input_size'],\n",
    "            batch_size=args.batch_size,\n",
    "            is_training=True,\n",
    "            use_prefetcher=args.prefetcher,\n",
    "            no_aug=args.no_aug,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            re_split=args.resplit,\n",
    "            scale=args.scale,\n",
    "            ratio=args.ratio,\n",
    "            hflip=args.hflip,\n",
    "            vflip=args.vflip,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            num_aug_splits=num_aug_splits,\n",
    "            interpolation=train_interpolation,\n",
    "            mean=data_config['mean'],\n",
    "            std=data_config['std'],\n",
    "            num_workers=args.workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=args.pin_mem,\n",
    "        )\n",
    "\n",
    "        loader_eval = create_loader(\n",
    "            dataset_eval,\n",
    "            input_size=data_config['input_size'],\n",
    "            batch_size=args.validation_batch_size_multiplier * args.batch_size,\n",
    "            is_training=False,\n",
    "            use_prefetcher=args.prefetcher,\n",
    "            interpolation=data_config['interpolation'],\n",
    "            mean=data_config['mean'],\n",
    "            std=data_config['std'],\n",
    "            num_workers=args.workers,\n",
    "            crop_pct=data_config['crop_pct'],\n",
    "            pin_memory=args.pin_mem,\n",
    "        )\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing).cuda()\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform\n",
    "        train_loss_fn = SoftTargetCrossEntropy().cuda()\n",
    "\n",
    "    elif args.smoothing:\n",
    "        train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing).cuda()\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    decreasing = True if eval_metric == 'loss' else False\n",
    "\n",
    "    saver = CheckpointSaver(\n",
    "        model=model, optimizer=optimizer, args=args, amp_scaler=loss_scaler,\n",
    "        checkpoint_dir=output_dir, recovery_dir=output_dir, decreasing=decreasing, max_history=args.checkpoint_hist)\n",
    "    with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "        f.write(args_text)\n",
    "    \"\"\"\n",
    "\n",
    "    saver = Saver(\n",
    "        model=model, optimizer=optimizer, args=args, amp_scaler=loss_scaler,\n",
    "        checkpoint_dir=output_dir, recovery_dir=output_dir, decreasing=decreasing, max_history=args.checkpoint_hist)\n",
    "    with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "        f.write(args_text)\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss_history, val_loss_history, train_acc_history, val_acc_history = [], [], [], []\n",
    "    if args.resume:\n",
    "        train_loss_history, val_loss_history, val_acc_history = read_history_from_csv(os.path.dirname(args.resume))\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch, model, loader_train, optimizer, train_loss_fn, args,\n",
    "                lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast, loss_scaler=loss_scaler, mixup_fn=mixup_fn)\n",
    "\n",
    "            eval_metrics = validate(model, loader_eval, validate_loss_fn, args, amp_autocast=amp_autocast)\n",
    "\n",
    "            if args.save_graphs:\n",
    "                train_loss_history.append(train_metrics[\"loss\"])\n",
    "                val_loss_history.append(eval_metrics[\"loss\"])\n",
    "                val_acc_history.append(eval_metrics[\"top1\"])\n",
    "                update_graph(train_loss_history, val_loss_history, val_acc_history, os.path.join(output_dir, \"graphs\"))\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "            if output_dir is not None:\n",
    "                update_summary(\n",
    "                    epoch, train_metrics, eval_metrics, os.path.join(output_dir, 'summary.csv'),\n",
    "                    write_header=best_metric is None)\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        epoch, model, loader, optimizer, loss_fn, args,\n",
    "        lr_scheduler=None, saver=None, output_dir=None, amp_autocast=suppress,\n",
    "        loss_scaler=None, mixup_fn=None):\n",
    "    if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n",
    "        if args.prefetcher and loader.mixup_enabled:\n",
    "            loader.mixup_enabled = False\n",
    "        elif mixup_fn is not None:\n",
    "            mixup_fn.mixup_enabled = False\n",
    "\n",
    "    second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    num_updates = epoch * len(loader)\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "\n",
    "        last_batch = batch_idx == last_idx\n",
    "        data_time_m.update(time.time() - end)\n",
    "\n",
    "        with amp_autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "\n",
    "        if isinstance(input, tuple):\n",
    "            input_size = input[0].size(0)\n",
    "        else:\n",
    "            input_size = input.size(0)\n",
    "\n",
    "        losses_m.update(loss.item(), input_size)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if loss_scaler is not None:\n",
    "            loss_scaler(\n",
    "                loss, optimizer,\n",
    "                clip_grad=args.clip_grad, clip_mode=args.clip_mode,\n",
    "                parameters=model_parameters(model, exclude_head='agc' in args.clip_mode),\n",
    "                create_graph=second_order)\n",
    "        else:\n",
    "            loss.backward(create_graph=second_order)\n",
    "            if args.clip_grad is not None:\n",
    "                dispatch_clip_grad(\n",
    "                    model_parameters(model, exclude_head='agc' in args.clip_mode),\n",
    "                    value=args.clip_grad, mode=args.clip_mode)\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        num_updates += 1\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        if last_batch or batch_idx % args.log_interval == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "            _logger.info(\n",
    "                'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
    "                'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
    "                'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
    "                '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                'LR: {lr:.3e}  '\n",
    "                'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
    "                    epoch,\n",
    "                    batch_idx, len(loader),\n",
    "                    100. * batch_idx / last_idx,\n",
    "                    loss=losses_m,\n",
    "                    batch_time=batch_time_m,\n",
    "                    rate=input_size * args.world_size / batch_time_m.val,\n",
    "                    rate_avg=input_size * args.world_size / batch_time_m.avg,\n",
    "                    lr=lr,\n",
    "                    data_time=data_time_m))\n",
    "\n",
    "            if args.save_images and output_dir:\n",
    "                torchvision.utils.save_image(\n",
    "                    input,\n",
    "                    os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n",
    "                    padding=0,\n",
    "                    normalize=True)\n",
    "\n",
    "        if saver is not None and args.recovery_interval and (\n",
    "                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n",
    "            saver.save_recovery(epoch, batch_idx=batch_idx)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "        end = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])\n",
    "\n",
    "\n",
    "def validate(model, loader, loss_fn, args, amp_autocast=suppress, log_suffix=''):\n",
    "    batch_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "    top1_m = AverageMeter()\n",
    "    top5_m = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            last_batch = batch_idx == last_idx\n",
    "            if not args.prefetcher:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            if args.channels_last:\n",
    "                input = input.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "            if isinstance(output, (tuple, list)):\n",
    "                output = output[0]\n",
    "\n",
    "            # augmentation reduction\n",
    "            reduce_factor = args.tta\n",
    "            if reduce_factor > 1:\n",
    "                output = output.unfold(0, reduce_factor, reduce_factor).mean(dim=2)\n",
    "                target = target[0:target.size(0):reduce_factor]\n",
    "\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "            reduced_loss = loss.data\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            if isinstance(input, tuple):\n",
    "                input_size = input[0].size(0)\n",
    "            else:\n",
    "                input_size = input.size(0)\n",
    "            losses_m.update(reduced_loss.item(), input_size)\n",
    "            top1_m.update(acc1.item(), output.size(0))\n",
    "            top5_m.update(acc5.item(), output.size(0))\n",
    "\n",
    "            batch_time_m.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if (last_batch or batch_idx % args.log_interval == 0):\n",
    "                log_name = 'Test' + log_suffix\n",
    "                _logger.info(\n",
    "                    '{0}: [{1:>4d}/{2}]  '\n",
    "                    'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n",
    "                    'Acc@01: {top1.val:>7.4f} ({top1.avg:>7.4f})  '\n",
    "                    'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})'.format(\n",
    "                        log_name, batch_idx, last_idx, batch_time=batch_time_m,\n",
    "                        loss=losses_m, top1=top1_m, top5=top5_m))\n",
    "\n",
    "    metrics = OrderedDict([('loss', losses_m.avg), ('top1', top1_m.avg), ('top5', top5_m.avg)])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    setup_default_logging()\n",
    "    args, args_text = _parse_args()\n",
    "    model_name = \"\"\n",
    "    if args.pretrained:\n",
    "        model_name = '-'.join(\n",
    "            [safe_model_name(args.model), 'pretrained'])\n",
    "    else:\n",
    "        model_name = '-'.join(\n",
    "            [safe_model_name(args.model), 'scratch'])\n",
    "\n",
    "    dataset = get_dataset_name_from_path(args.data_train_dir)\n",
    "    num_classes_experiment = get_classes_name_from_path(args.data_train_dir)\n",
    "    aug_type = get_aug_type_from_path(args.data_train_dir)\n",
    "    exp_type = get_exp_type_from_path(args.data_train_dir)\n",
    "\n",
    "    assert exp_type == \"within-corpus\" or exp_type == \"IEMOCAP_train\" or exp_type == \"cross-corpus\"\n",
    "\n",
    "    root_output_dir = os.path.join(args.root_output_dir, exp_type, model_name, dataset,\n",
    "                                   num_classes_experiment, aug_type)\n",
    "    class_to_idx = get_class_to_idx(dataset, num_classes_experiment)\n",
    "    folders = [os.path.basename(f.path) for f in os.scandir(args.data_train_dir) if f.is_dir()]\n",
    "\n",
    "    if exp_type == \"within-corpus\":\n",
    "        for idx, subject in enumerate(folders):\n",
    "            print(\"TRAIN MODELS FOR TEST FOLDER \" + str(idx + 1) + \"/\" + str(len(folders)))\n",
    "            output_subject_dir = os.path.join(root_output_dir, subject)\n",
    "            subject_dir = os.path.join(args.data_train_dir, subject, \"train\", \"datasets\")\n",
    "            subjects_val = [os.path.basename(f.path) for f in os.scandir(subject_dir) if f.is_dir()]\n",
    "            for idx_val, subject_val in enumerate(subjects_val):\n",
    "                print(\"----TRAIN MODELS FOR TEST FOLDER \" + str(idx + 1) + \"/\" + str(len(folders)) + \": VALIDATION \" + str(\n",
    "                    idx_val + 1) + \"/\" + str(len(subjects_val)))\n",
    "                current_train_csv = os.path.join(subject_dir, subject_val, \"train\", \"dataset.csv\")\n",
    "                current_eval_csv = os.path.join(subject_dir, subject_val, \"validation\", \"dataset.csv\")\n",
    "                output_sub_subject_dir = os.path.join(output_subject_dir, subject_val)\n",
    "                if os.path.exists(output_sub_subject_dir):\n",
    "                    continue\n",
    "                else:\n",
    "                    os.makedirs(output_sub_subject_dir)\n",
    "                    train(current_train_csv, current_eval_csv, output_sub_subject_dir, args, class_to_idx)\n",
    "\n",
    "    elif exp_type == \"IEMOCAP_train\":\n",
    "        for idx, subject in enumerate(folders):\n",
    "            print(\"TRAIN MODELS FOR TEST FOLDER \" + str(idx + 1) + \"/\" + str(len(folders)))\n",
    "            output_subject_dir = os.path.join(root_output_dir, subject)\n",
    "            subject_train_csv = os.path.join(args.data_train_dir, subject, \"train\", \"dataset.csv\")\n",
    "            subject_val_csv = os.path.join(args.data_train_dir, subject, \"validation\", \"dataset.csv\")\n",
    "            if os.path.exists(output_subject_dir):\n",
    "                continue\n",
    "            else:\n",
    "                os.makedirs(output_subject_dir)\n",
    "                train(subject_train_csv, subject_val_csv, output_subject_dir, args, class_to_idx)\n",
    "\n",
    "    elif exp_type == \"cross-corpus\":\n",
    "        for idx, corpus in enumerate(folders):\n",
    "            print(\"TRAIN MODELS FOR TEST FOLDER \" + str(idx + 1) + \"/\" + str(len(folders)))\n",
    "            output_corpus_dir = os.path.join(root_output_dir, corpus)\n",
    "            corpus_train_csv = os.path.join(args.data_train_dir, corpus, \"train\", \"dataset.csv\")\n",
    "            corpus_eval_csv = os.path.join(args.data_train_dir, corpus, \"validation\", \"dataset.csv\")\n",
    "\n",
    "            if os.path.exists(output_corpus_dir):\n",
    "                continue\n",
    "            else:\n",
    "                os.makedirs(output_corpus_dir)\n",
    "                train(corpus_train_csv, corpus_eval_csv, output_corpus_dir, args, class_to_idx)\n",
    "\n",
    "    else:\n",
    "        exit(0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_z3voK0URs3x",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1648655483368,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Alessandro Arezzo",
      "userId": "13411823202273040064"
     }
    },
    "outputId": "622406e1-694a-473f-d1ac-3ca2dbd823d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EVAL WITHIN CORPUS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile eval_within_corpus.py\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from timm.data import create_loader, resolve_data_config, ImageDataset\n",
    "from timm.models import create_model\n",
    "\n",
    "import torch\n",
    "\n",
    "from custom_loader import create_custom_loader\n",
    "from parser_csv import ParserCSV\n",
    "from speaker_vgg_dataset import SpeakerVGGDataset\n",
    "from utils import save_performance, save_confusion_matrix, get_class_labels\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from src import *\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import logging\n",
    "\n",
    "# Model to test arguments\n",
    "\n",
    "config_parser = parser = argparse.ArgumentParser(description='Training Config', add_help=False)\n",
    "\"\"\"\n",
    "parser.add_argument('-c', '--config', default='', type=str, metavar='FILE',\n",
    "                    help='YAML config file specifying default arguments')\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='PyTorch Timm Tests')\n",
    "# Dataset / Model parameters\n",
    "parser.add_argument('--data_eval_dir', metavar='DIR',\n",
    "                    help='path to validation')\n",
    "parser.add_argument('--speaker_embedder_path', metavar='DIR',\n",
    "                    help='path to speaker embedder for speaker_cct')\n",
    "parser.add_argument('--gender_embedder_path', metavar='DIR',\n",
    "                    help='path to gender embedder for speaker_cct')\n",
    "parser.add_argument('--corpus_embedder_path', metavar='DIR',\n",
    "                    help='path to corpus embedder for speaker_cct')\n",
    "parser.add_argument('--model', metavar='NAME', type=str,\n",
    "                    help='name model timm')\n",
    "parser.add_argument('--dataset', '-d', metavar='NAME', default='',\n",
    "                    help='dataset type (default: ImageFolder/ImageTar if empty)')\n",
    "parser.add_argument('--num-classes', type=int, default=None, metavar='N',\n",
    "                    help='number of label classes (Model default if None)')\n",
    "parser.add_argument('--gp', default=None, type=str, metavar='POOL',\n",
    "                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\n",
    "parser.add_argument('--img-size', type=int, default=None, metavar='N',\n",
    "                    help='Image patch size (default: None => model default)')\n",
    "parser.add_argument('--input-size', default=None, nargs=3, type=int,\n",
    "                    metavar='N N N',\n",
    "                    help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\n",
    "parser.add_argument('--crop-pct', default=None, type=float,\n",
    "                    metavar='N', help='Input image center crop percent (for validation only)')\n",
    "parser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n",
    "                    help='Override mean pixel value of dataset')\n",
    "parser.add_argument('--std', type=float, nargs='+', default=None, metavar='STD',\n",
    "                    help='Override std deviation of of dataset')\n",
    "# Augmentation & regularization parameters\n",
    "parser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                    help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop-connect', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop connect rate, DEPRECATED, use drop-path (default: None)')\n",
    "parser.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop path rate (default: None)')\n",
    "parser.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n",
    "                    help='Drop block rate (default: None)')\n",
    "# Batch norm parameters (only works with gen_efficientnet based models currently)\n",
    "parser.add_argument('--bn-tf', action='store_true', default=False,\n",
    "                    help='Use Tensorflow BatchNorm defaults for models that support it (default: False)')\n",
    "parser.add_argument('--bn-momentum', type=float, default=None,\n",
    "                    help='BatchNorm momentum override (if not None)')\n",
    "parser.add_argument('--bn-eps', type=float, default=None,\n",
    "                    help='BatchNorm epsilon override (if not None)')\n",
    "# Misc\n",
    "parser.add_argument('--pin-mem', action='store_true', default=False,\n",
    "                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "\n",
    "\n",
    "# Script arguments\n",
    "\n",
    "parser.add_argument('model_path', metavar='PATH', type=str,\n",
    "                    help='model to test relative path')\n",
    "parser.add_argument('--output_dir', default= \"./\", metavar='PATH', type=str,\n",
    "                    help='root outputu dir')\n",
    "parser.add_argument('--experiment', metavar='NAME', type=str,\n",
    "                    help='experiment type (research | production | cross-corpus)')\n",
    "parser.add_argument('--augmentation', metavar='NAME',\n",
    "                    help='augmentation type (no_aug | time_aug | freq_aug | time_freq_aug)')\n",
    "parser.add_argument('--pretrained', metavar='NAME', type=str, default='scratch',\n",
    "                    help='model pretrained (pretrained | scratch)')\n",
    "parser.add_argument('--data_test_path', metavar='PATH', type=str, default='',\n",
    "                    help='test data absolute path')\n",
    "parser.add_argument('--interpolation', metavar='NAME', type=str, default='bicubic',\n",
    "                    help='resize interpolation')\n",
    "parser.add_argument('--num_workers', metavar='NAME', type=int, default=4,\n",
    "                    help='number of workers')\n",
    "_logger = logging.getLogger('inference')\n",
    "\n",
    "def _parse_args(config_file):\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    args_config.config = config_file\n",
    "\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    first_folder = [f.path for f in os.scandir(args.model_path) if f.is_dir()][0]\n",
    "    config_file = os.path.join([f.path for f in os.scandir(first_folder) if f.is_dir()][0], \"args.yaml\")\n",
    "    args, args_text = _parse_args(config_file)\n",
    "    root_dir = \"./\"\n",
    "\n",
    "    pretrained_str = \"\"\n",
    "    if args.pretrained:\n",
    "        pretrained_str = \"pretrained\"\n",
    "    else:\n",
    "        pretrained_str = \"scratch\"\n",
    "\n",
    "    model = ('-').join([args.model, pretrained_str])\n",
    "    dataset = get_dataset_name_from_path(args.data_test_path)\n",
    "    num_classes_experiment = get_classes_name_from_path(args.data_test_path)\n",
    "    aug_type = get_aug_type_from_path(args.data_test_path)\n",
    "    exp_type = get_exp_type_from_path(args.data_test_path)\n",
    "\n",
    "    assert exp_type == \"within-corpus\" or exp_type == \"IEMOCAP_train\" or exp_type == \"cross-corpus\"\n",
    "\n",
    "    output_dir = os.path.join(args.output_dir, \"test\", exp_type, model, dataset,\n",
    "                              num_classes_experiment, aug_type)\n",
    "    class_to_idx = get_class_to_idx(dataset, num_classes_experiment)\n",
    "\n",
    "    class_labels = get_class_labels(args.data_train_dir)\n",
    "\n",
    "    class_labels_in_order = {}\n",
    "    for c in class_labels:\n",
    "        class_labels_in_order[c] = class_to_idx[c]\n",
    "    class_labels_in_order = {k: v for k, v in sorted(class_labels_in_order.items(), key=lambda item: item[1])}\n",
    "\n",
    "    folders = [os.path.basename(f.path) for f in os.scandir(args.data_test_path) if f.is_dir()]\n",
    "\n",
    "    accuracy_tot, micro_precision_tot, macro_precision_tot, micro_recall_tot, macro_recall_tot, \\\n",
    "    micro_f1_tot, macro_f1_tot = 0, 0, 0, 0, 0, 0, 0\n",
    "    cm_tot = np.zeros((args.num_classes, args.num_classes))\n",
    "\n",
    "    for idx, folder in enumerate(folders):\n",
    "        print(\"PROCESS FOLDER \" + str(idx+1) + \"/\" + str(len(folders)))\n",
    "        performance_output = os.path.join(output_dir, folder)\n",
    "\n",
    "        if not os.path.exists(performance_output):\n",
    "            os.makedirs(performance_output)\n",
    "\n",
    "        performance_output_file_path = os.path.join(performance_output, \"performance.yaml\")\n",
    "        cm_output_file_path = os.path.join(performance_output, \"conf_matrix.png\")\n",
    "\n",
    "        eval_csv = os.path.join(args.data_test_path, folder, \"test\", \"dataset.csv\")\n",
    "        eval_models_dirs = [f.path for f in os.scandir(os.path.join(args.model_path, folder)) if f.is_dir()]\n",
    "\n",
    "        accuracy_folder,  micro_precision_folder, macro_precision_folder, micro_recall_folder, macro_recall_folder,\\\n",
    "        micro_f1_folder, macro_f1_folder = 0, 0, 0, 0, 0, 0, 0\n",
    "        cm_folder = np.zeros((args.num_classes, args.num_classes))\n",
    "\n",
    "        for eval_model_dir in eval_models_dirs:\n",
    "\n",
    "            eval_model_path = os.path.join(eval_model_dir, \"model_best.pth.tar\")\n",
    "            if not os.path.exists(eval_csv):\n",
    "                print(\"TEST DATASET \" + eval_csv + \" NOT EXISTS\")\n",
    "                return\n",
    "\n",
    "            if not os.path.exists(eval_model_path):\n",
    "                print(\"MODEL NOT FIND: \" + eval_model_path)\n",
    "                return\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "            model = create_model(\n",
    "                args.model,\n",
    "                num_classes=args.num_classes,\n",
    "                checkpoint_path=eval_model_path,\n",
    "                pretrained=args.pretrained,\n",
    "                drop_rate=args.drop,\n",
    "                drop_connect_rate=args.drop_connect,\n",
    "                drop_path_rate=args.drop_path,\n",
    "                drop_block_rate=args.drop_block,\n",
    "                global_pool=args.gp,\n",
    "                bn_tf=args.bn_tf,\n",
    "                bn_momentum=args.bn_momentum,\n",
    "                bn_eps=args.bn_eps,\n",
    "                speaker_embedder_path=args.speaker_embedder_path,\n",
    "                gender_embedder_path=args.gender_embedder_path,\n",
    "                corpus_embedder_path=args.corpus_embedder_path)\n",
    "\n",
    "            config = resolve_data_config(vars(args), model=model)\n",
    "            model.cuda()\n",
    "\n",
    "            first_part_model_name = args.model.split('_')[0]\n",
    "            second_part_model_name = args.model.split('_')[1]\n",
    "            read_vgg_features = False\n",
    "\n",
    "            # create the train and eval datasets\n",
    "            \"\"\"\n",
    "            if first_part_model_name == \"hybrid\":\n",
    "                read_wav = True\n",
    "                dataset_train = HybridDataset(str(train_csv),\n",
    "                                             parser=ParserCSV(train_csv, class_to_idx=class_to_idx, read_wav=True))\n",
    "                dataset_eval = HybridDataset(str(eval_csv),\n",
    "                                            parser=ParserCSV(eval_csv, class_to_idx=class_to_idx, read_wav=True))\n",
    "            \"\"\"\n",
    "            if first_part_model_name == \"speaker\" and second_part_model_name == \"vgg\":\n",
    "                read_vgg_features = True\n",
    "                dataset_test = SpeakerVGGDataset(str(eval_csv), parser=ParserCSV(eval_csv, class_to_idx=class_to_idx,\n",
    "                                                                                 read_vgg_features=True))\n",
    "            else:\n",
    "                dataset_test = ImageDataset(str(eval_csv), parser=ParserCSV(eval_csv, class_to_idx=class_to_idx))\n",
    "            if read_vgg_features:\n",
    "                loader = create_custom_loader(\n",
    "                    dataset_test,\n",
    "                    input_size=config['input_size'],\n",
    "                    batch_size=1,\n",
    "                    use_prefetcher=True,\n",
    "                    interpolation=config['interpolation'],\n",
    "                    mean=config['mean'],\n",
    "                    std=config['std'],\n",
    "                    num_workers=args.num_workers,\n",
    "                    crop_pct=config['crop_pct'])\n",
    "            else:\n",
    "                loader = create_loader(\n",
    "                    dataset_test,\n",
    "                    input_size=config['input_size'],\n",
    "                    batch_size=1,\n",
    "                    use_prefetcher=True,\n",
    "                    interpolation=config['interpolation'],\n",
    "                    mean=config['mean'],\n",
    "                    std=config['std'],\n",
    "                    num_workers=args.num_workers,\n",
    "                    crop_pct=config['crop_pct'])\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y_test_true, y_test_predicted = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (input, target) in enumerate(loader):\n",
    "                    if isinstance(input, tuple):\n",
    "                        input = (input[0].cuda(), input[1].cuda())\n",
    "                    else:\n",
    "                        input = input.cuda()\n",
    "                    target = target.cuda()\n",
    "                    labels = model(input)\n",
    "                    top1 = labels.topk(1)[1].cpu().numpy()\n",
    "\n",
    "                    y_test_true.append(target.cpu().numpy()[0])\n",
    "                    y_test_predicted.append(top1[0][0])\n",
    "\n",
    "\n",
    "            y_test_true = np.array(y_test_true)\n",
    "            y_test_predicted = np.array(y_test_predicted)\n",
    "\n",
    "            # calcolo le statistiche\n",
    "            accuracy_folder += accuracy_score(y_test_true, y_test_predicted)\n",
    "            micro_precision_folder += precision_score(y_test_true, y_test_predicted, average=\"micro\")\n",
    "            macro_precision_folder += precision_score(y_test_true, y_test_predicted, average=\"macro\")\n",
    "            micro_recall_folder += recall_score(y_test_true, y_test_predicted, average=\"micro\")\n",
    "            macro_recall_folder += recall_score(y_test_true, y_test_predicted, average=\"macro\")\n",
    "            micro_f1_folder += f1_score(y_test_true, y_test_predicted, average=\"micro\")\n",
    "            macro_f1_folder += f1_score(y_test_true, y_test_predicted, average=\"macro\")\n",
    "\n",
    "            cm_folder += confusion_matrix(y_test_true, y_test_predicted, labels=[class_labels_in_order[c] for c in class_labels_in_order],\n",
    "                                          normalize='true')\n",
    "            cm_folder[np.isnan(cm_folder)] = 0\n",
    "\n",
    "            #report = classification_report(y_test_true, y_test_predicted)\n",
    "\n",
    "        accuracy = accuracy_folder / len(eval_models_dirs)\n",
    "        micro_precision = micro_precision_folder / len(eval_models_dirs)\n",
    "        macro_precision = macro_precision_folder / len(eval_models_dirs)\n",
    "        micro_recall = micro_recall_folder / len(eval_models_dirs)\n",
    "        macro_recall = macro_recall_folder / len(eval_models_dirs)\n",
    "        micro_f1 = micro_f1_folder / len(eval_models_dirs)\n",
    "        macro_f1 = macro_f1_folder / len(eval_models_dirs)\n",
    "\n",
    "        cm = cm_folder / len(eval_models_dirs)\n",
    "        sum_of_rows = cm.sum(axis=1)\n",
    "        cm = cm / sum_of_rows[:, np.newaxis]\n",
    "        cm[np.isnan(cm)] = 0\n",
    "\n",
    "        model_stats = {\"accuracy\": str(accuracy * 100),\n",
    "                       \"micro_precision\": str(micro_precision * 100),\n",
    "                       \"micro_recall\": str(micro_recall * 100),\n",
    "                       \"micro_f1\": str(micro_f1 * 100),\n",
    "                       \"macro_precision\": str(macro_precision * 100),\n",
    "                       \"macro_recall\": str(macro_recall * 100),\n",
    "                       \"macro_f1\": str(macro_f1 * 100), }\n",
    "\n",
    "        accuracy_tot += accuracy\n",
    "        micro_precision_tot += micro_precision\n",
    "        macro_precision_tot += macro_precision\n",
    "        micro_recall_tot += micro_recall\n",
    "        macro_recall_tot += macro_recall\n",
    "        micro_f1_tot += micro_f1\n",
    "        macro_f1_tot += macro_f1\n",
    "\n",
    "        cm_tot += cm\n",
    "\n",
    "        print(model_stats)\n",
    "\n",
    "        save_performance(model_stats, performance_output_file_path)\n",
    "        print(\"Performance folder \" + str(len(folders)) + \" saved on: \" + performance_output_file_path)\n",
    "\n",
    "        save_confusion_matrix(cm, class_labels_in_order, cm_output_file_path)\n",
    "        print(\"Confusion matrix model saved on: \" + cm_output_file_path)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_tot / len(folders)\n",
    "    micro_precision = micro_precision_tot / len(folders)\n",
    "    micro_recall = macro_precision_tot / len(folders)\n",
    "    micro_f1 = micro_recall_tot / len(folders)\n",
    "    macro_precision = macro_recall_tot / len(folders)\n",
    "    macro_recall = micro_f1_tot / len(folders)\n",
    "    macro_f1 = macro_f1_tot / len(folders)\n",
    "\n",
    "    cm = cm_tot / len(folders)\n",
    "    sum_of_rows = cm.sum(axis=1)\n",
    "    cm = cm / sum_of_rows[:, np.newaxis]\n",
    "\n",
    "\n",
    "    model_stats = {\"accuracy\": str(accuracy * 100),\n",
    "                   \"micro_precision\": str(micro_precision * 100),\n",
    "                   \"micro_recall\": str(micro_recall * 100),\n",
    "                   \"micro_f1\": str(micro_f1 * 100),\n",
    "                   \"macro_precision\": str(macro_precision * 100),\n",
    "                   \"macro_recall\": str(macro_recall * 100),\n",
    "                   \"macro_f1\": str(macro_f1 * 100), }\n",
    "\n",
    "    performance_output_file_path = os.path.join(output_dir, \"performance.yaml\")\n",
    "    save_performance(model_stats, performance_output_file_path)\n",
    "    print(\"Performance model saved on: \" + performance_output_file_path)\n",
    "\n",
    "    cm_output_file_path = os.path.join(output_dir, \"conf_matrix.png\")\n",
    "    save_confusion_matrix(cm, class_labels_in_order, cm_output_file_path)\n",
    "    print(\"Confusion matrix model saved on: \" + cm_output_file_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUN SCRIPTS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python train.py /content/drive/MyDrive/datasets/within-corpus_csv/EMODB/all_classes/no_aug/datasets -c /content/drive/MyDrive/Colab\\ Notebooks/Models/Compact-Transformers/configs/pretrained/cct_7-3x1_cifar100_300epochs.yml --root_output_dir /content/drive/MyDrive/output/train/ --pretrained --model speaker_vgg_cct_7_3x1_32 --num-classes 6 --input-size 3 32 32 --batch-size 32 --dataset '' --no-aug --workers 2 --checkpoint-hist 1 --save_graphs --epochs 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python train.py /content/drive/MyDrive/datasets/within-corpus_csv/EMODB/all_classes/no_aug/datasets -c /content/drive/MyDrive/Colab\\ Notebooks/Models/Compact-Transformers/configs/pretrained/cct_7-3x1_cifar100_300epochs.yml --pretrained --model cct_7_3x1_32 --num-classes 6 --input-size 3 32 32 --batch-size 32 --dataset '' --no-aug --workers 2 --checkpoint-hist 1 --save_graphs --epochs 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python train.py /home/ale-ubuntu/Programming_workspace/Datasets/within-corpus/EMODB/all_classes/no_aug -c ../configs/pretrained/cct_7-3x1_cifar100_300epochs.yml --root_output_dir ../output/train/ --pretrained --model speaker_vgg_cct_7_3x1_32 --num-classes 6 --input-size 3 32 32 --batch-size 32 --dataset '' --workers 8 --checkpoint-hist 1 --save_graphs --epochs 200 --no-aug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python eval_within_corpus.py ../output/train/within-corpus/speaker_vgg_cct_7_3x1_32-pretrained/EMODB/all_classes/no_aug/ --data_test_path /home/ale-ubuntu/Programming_workspace/Datasets/within-corpus/EMODB/all_classes/no_aug --output_dir ../"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}